<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>9927cb3d21584608bd85ca5c84597dfd</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div id="67d120a3" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;689b71232e2b2445d032d7f28d8b8f3b&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-412524340acc422c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h1 id="laboratorio-3">Laboratorio 3</h1>
<p>Sean bienvenidos de nuevo al laboratorio 3 de Deep Learning y
Sistemas Inteligentes. Así como en los laboratorios pasados, espero que
esta ejercitación les sirva para consolidar sus conocimientos en el tema
de Redes Neuronales Recurrentes y LSTM.</p>
<p>Este laboratorio consta de dos partes. En la primera trabajaremos una
Red Neuronal Recurrente paso-a-paso. En la segunda fase, usaremos
PyTorch para crear una nueva Red Neuronal pero con LSTM, con la
finalidad de que no solo sepan que existe cierta función sino también
entender qué hace en un poco más de detalle.</p>
<p>Para este laboratorio estaremos usando una herramienta para Jupyter
Notebooks que facilitará la calificación, no solo asegurándo que ustedes
tengan una nota pronto sino también mostrandoles su nota final al
terminar el laboratorio.</p>
<p>Espero que esta vez si se muestren los <em>marks</em>. De nuevo me
discupo si algo no sale bien, seguiremos mejorando conforme vayamos
iterando. Siempre pido su comprensión y colaboración si algo no funciona
como debería.</p>
<p>Al igual que en el laboratorio pasado, estaremos usando la librería
de Dr John Williamson et al de la University of Glasgow, además de
ciertas piezas de código de Dr Bjorn Jensen de su curso de Introduction
to Data Science and System de la University of Glasgow para la
visualización de sus calificaciones.</p>
<p><strong>NOTA:</strong> Ahora tambien hay una tercera dependecia que
se necesita instalar. Ver la celda de abajo por favor</p>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</div>
<div id="b4fa65b0" class="cell code" data-execution_count="1"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:13.426912Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:13.420034Z&quot;}">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Una vez instalada la librería por favor, recuerden volverla a comentar.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install <span class="op">-</span>U <span class="op">--</span>force<span class="op">-</span>reinstall <span class="op">--</span>no<span class="op">-</span>cache https:<span class="op">//</span>github.com<span class="op">/</span>johnhw<span class="op">/</span>jhwutils<span class="op">/</span>zipball<span class="op">/</span>master</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install scikit<span class="op">-</span>image</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install <span class="op">-</span>U <span class="op">--</span>force<span class="op">-</span>reinstall <span class="op">--</span>no<span class="op">-</span>cache https:<span class="op">//</span>github.com<span class="op">/</span>AlbertS789<span class="op">/</span>lautils<span class="op">/</span>zipball<span class="op">/</span>master</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting https://github.com/johnhw/jhwutils/zipball/master
  Downloading https://github.com/johnhw/jhwutils/zipball/master
     - 0 bytes ? 0:00:00
     - 11.6 kB ? 0:00:00
     \ 38.1 kB 369.0 kB/s 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status &#39;done&#39;
Building wheels for collected packages: jhwutils
  Building wheel for jhwutils (setup.py): started
  Building wheel for jhwutils (setup.py): finished with status &#39;done&#39;
  Created wheel for jhwutils: filename=jhwutils-1.0-py3-none-any.whl size=33803 sha256=66c465f45e8161dd31f692f63b4f8549eea46f620e84122ff79801e1e4bb97d5
  Stored in directory: C:\Users\charl\AppData\Local\Temp\pip-ephem-wheel-cache-pdot5pqt\wheels\27\3c\cb\eb7b3c6ea36b5b54e5746751443be9bb0d73352919033558a2
Successfully built jhwutils
Installing collected packages: jhwutils
  Attempting uninstall: jhwutils
    Found existing installation: jhwutils 1.0
    Uninstalling jhwutils-1.0:
      Successfully uninstalled jhwutils-1.0
Successfully installed jhwutils-1.0
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
[notice] A new release of pip is available: 23.0.1 -&gt; 23.2.1
[notice] To update, run: C:\Users\charl\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\python.exe -m pip install --upgrade pip
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Requirement already satisfied: scikit-image in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (0.21.0)
Requirement already satisfied: networkx&gt;=2.8 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (3.1)
Requirement already satisfied: pillow&gt;=9.0.1 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (10.0.0)
Requirement already satisfied: packaging&gt;=21 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (23.1)
Requirement already satisfied: PyWavelets&gt;=1.1.1 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (1.4.1)
Requirement already satisfied: tifffile&gt;=2022.8.12 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (2023.7.18)
Requirement already satisfied: numpy&gt;=1.21.1 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (1.24.3)
Requirement already satisfied: lazy_loader&gt;=0.2 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (0.3)
Requirement already satisfied: imageio&gt;=2.27 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (2.31.1)
Requirement already satisfied: scipy&gt;=1.8 in c:\users\charl\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from scikit-image) (1.11.1)
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
[notice] A new release of pip is available: 23.0.1 -&gt; 23.2.1
[notice] To update, run: C:\Users\charl\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\python.exe -m pip install --upgrade pip
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Collecting https://github.com/AlbertS789/lautils/zipball/master
  Downloading https://github.com/AlbertS789/lautils/zipball/master
     - 0 bytes ? 0:00:00
     - 4.1 kB ? 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status &#39;done&#39;
Building wheels for collected packages: lautils
  Building wheel for lautils (setup.py): started
  Building wheel for lautils (setup.py): finished with status &#39;done&#39;
  Created wheel for lautils: filename=lautils-1.0-py3-none-any.whl size=2704 sha256=c8143484d632e8fb6c5776ca6495c6fd8174e9d7bf68b0f2539bb07946b97ae6
  Stored in directory: C:\Users\charl\AppData\Local\Temp\pip-ephem-wheel-cache-pojrt_xd\wheels\16\3a\a0\5fbae86e17ef6bb8ed057aa04b591584005d1212c72d69fc70
Successfully built lautils
Installing collected packages: lautils
  Attempting uninstall: lautils
    Found existing installation: lautils 1.0
    Uninstalling lautils-1.0:
      Successfully uninstalled lautils-1.0
Successfully installed lautils-1.0
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
[notice] A new release of pip is available: 23.0.1 -&gt; 23.2.1
[notice] To update, run: C:\Users\charl\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\python.exe -m pip install --upgrade pip
</code></pre>
</div>
</div>
<div id="53d221fd" class="cell code" data-execution_count="2"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.491024Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:13.426912Z&quot;}">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">#from IPython import display</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">#from base64 import b64decode</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Other imports</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unittest.mock <span class="im">import</span> patch</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> uuid <span class="im">import</span> getnode <span class="im">as</span> get_mac</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jhwutils.checkarr <span class="im">import</span> array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.image_audio <span class="im">as</span> ia</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.tick <span class="im">as</span> tick</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lautils.gradeutils <span class="im">import</span> new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">###</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>tick.reset_marks()</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code></pre></div>
</div>
<div id="cf165e82" class="cell code" data-execution_count="3"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.506456Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.491024Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a39756cb52fe963f67e015d4d8fe57a4&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-57de155e9f3409c3&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seeds</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>seed_ <span class="op">=</span> <span class="dv">2023</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span></code></pre></div>
</div>
<div id="6688fc4e" class="cell code" data-execution_count="4"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.522082Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.506456Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;500bf8639033566b1f628a100f1180ca&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-e0ac5721852fe7fd&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Celda escondida para utlidades necesarias, por favor NO edite esta celda</span></span></code></pre></div>
</div>
<div id="eff949f8" class="cell markdown"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-07-30T00:51:50.240511Z&quot;,&quot;start_time&quot;:&quot;2023-07-30T00:51:50.231535Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;97d6b491fefaa9d0c1ffc3ac064a24bc&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-cdc148943062b4ab&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h6 id="información-del-estudiante-en-dos-variables">Información del
estudiante en dos variables</h6>
<ul>
<li>carne_1 : un string con su carne (e.g. "12281"), debe ser de al
menos 5 caracteres.</li>
<li>firma_mecanografiada_1: un string con su nombre (e.g. "Albero
Suriano") que se usará para la declaracion que este trabajo es propio
(es decir, no hay plagio)</li>
<li>carne_2 : un string con su carne (e.g. "12281"), debe ser de al
menos 5 caracteres.</li>
<li>firma_mecanografiada_2: un string con su nombre (e.g. "Albero
Suriano") que se usará para la declaracion que este trabajo es propio
(es decir, no hay plagio)</li>
</ul>
</div>
<div id="18be1d23" class="cell code" data-execution_count="5"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.537775Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.522082Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7cd4a99d7434f922d6754ac890fc97e5&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1dec8918a2e1a2cf&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>carne_1 <span class="op">=</span> <span class="st">&quot;20347&quot;</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_1 <span class="op">=</span> <span class="st">&quot;Alejandro Gómez&quot;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>carne_2 <span class="op">=</span>  <span class="st">&quot;20498&quot;</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_2 <span class="op">=</span> <span class="st">&quot;Gabriel Vicente&quot;</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">#raise NotImplementedError()</span></span></code></pre></div>
</div>
<div id="8d952cf0" class="cell code" data-execution_count="6"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.553326Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.537775Z&quot;}">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Deberia poder ver dos checkmarks verdes [0 marks], que indican que su información básica está OK </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>): </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(carne_1)<span class="op">&gt;=</span><span class="dv">5</span> <span class="kw">and</span> <span class="bu">len</span>(carne_2)<span class="op">&gt;=</span><span class="dv">5</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>):  </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(firma_mecanografiada_1)<span class="op">&gt;</span><span class="dv">0</span> <span class="kw">and</span> <span class="bu">len</span>(firma_mecanografiada_2)<span class="op">&gt;</span><span class="dv">0</span>)</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
</div>
<div id="4c6b6a3c" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;dc5db5a03eaba2adbf0f76c10e067442&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-3092f1f9ee984601&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h2 id="parte-1---construyendo-una-red-neuronal-recurrente">Parte 1 -
Construyendo una Red Neuronal Recurrente</h2>
<p><strong>Créditos:</strong> La primera parte de este laboratorio está
tomado y basado en uno de los laboratorios dados dentro del curso de
"Deep Learning" de Jes Frellsen (DeepLearningDTU)</p>
<p>La aplicación de los datos secuenciales pueden ir desde predicción
del clima hasta trabajar con lenguaje natural. En este laboratorio
daremos un vistazo a como las RNN pueden ser usadas dentro del modelaje
del lenguaje, es decir, trataremos de predecir el siguiente token dada
una secuencia. En el campo de NLP, un token puede ser un caracter o bien
una palabra.</p>
<h3 id="representanción-de-tokens-o-texto">Representanción de Tokens o
Texto</h3>
<p>Como bien hemos hablado varias veces, la computadora no entiende
palabras ni mucho menos oraciones completas en la misma forma que
nuestros cerebros lo hacen. Por ello, debemos encontrar alguna forma de
representar palabras o caracteres en una manera que la computadora sea
capaz de interpretarla, es decir, con números. Hay varias formas de
representar un grupo de palabras de forma numérica, pero para fines de
este laboratorio vamos a centrarnos en una manera común, llamada
"one-hot encoding".</p>
<h4 id="one-hot-encoding">One Hot Encoding</h4>
<p>Esta técnica debe resultarles familiar de cursos pasados, donde se
tomaba una conjunto de categorías y se les asignaba una columna por
categoría, entonces se coloca un 1 si el row que estamos evaluando es
parte de esa categoría o un 0 en caso contrario. Este mismo acercamiento
podemos tomarlo para representar conjuntos de palabras. Por ejemplo</p>
<pre><code>casa = [1, 0, 0, ..., 0]
perro = [0, 1, 0, ..., 0]</code></pre>
<p>Representar un vocabulario grande con one-hot enconding, suele
volverse ineficiente debido al tamaño de cada vector disperso. Para
solventar esto, una práctica común es truncar el vocabulario para
contener las palabras más utilizadas y representar el resto con un
símbolo especial, UNK, para definir palabras "desconocidas" o "sin
importancia". A menudo esto se hace que palabras tales como nombres se
vean como UNK porque son raros.</p>
<h3 id="generando-el-dataset-a-usar">Generando el Dataset a Usar</h3>
<p>Para este laboratorio usaremos un dataset simplificado, del cual
debería ser más sencillo el aprender de él. Estaremos generando
secuencias de la forma</p>
<pre><code>a b EOS
a a a a b b b b EOS</code></pre>
<p>Noten la aparición del token "EOS", el cual es un caracter especial
que denota el fin de la secuencia. Nuestro task en general será el
predecir el siguiente token <span
class="math inline"><em>t</em><sub><em>n</em></sub></span>, donde este
podrá ser "a", "b", "EOS", o "UNK" dada una secuencia de forma <span
class="math inline"><em>t</em><sub>1</sub>, ..., <em>t</em><sub><em>n</em> − 1</sub></span>.</p>
</div>
<div id="6cfc979b" class="cell code" data-execution_count="7"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.568947Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.553326Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;d0adba37e43168d88355edd44ad433cb&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-62b6e4727b9bb25c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reseed the cell</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data(num_seq<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Genera un grupo de secuencias, la cantidad de secuencias es dada por num_seq</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">    num_seq: El número de secuencias a ser generadas</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Una lista de secuencias</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> []</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_seq):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Genera una secuencia de largo aleatorio</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        num_tokens <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">12</span>) </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Genera la muestra</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> [<span class="st">&#39;a&#39;</span>] <span class="op">*</span> num_tokens <span class="op">+</span> [<span class="st">&#39;b&#39;</span>] <span class="op">*</span> num_tokens <span class="op">+</span> [<span class="st">&#39;EOS&#39;</span>]</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Agregamos</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        samples.append(sample)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> generate_data()</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Una secuencia del grupo generado&quot;</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sequences[<span class="dv">0</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Una secuencia del grupo generado
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
</code></pre>
</div>
</div>
<div id="dd08cda1" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;0a26144f688af47794960dfd5fdca804&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-844a1596734445c9&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h3 id="representación-de-tokens-como-índices">Representación de tokens
como índices</h3>
<p>En este paso haremos la parte del one-hot encoding. Para esto
necesitaremos asignar a cada posible palabra de nuestro vocabulario un
índice. Para esto crearemos dos diccionarios, uno que permitirá que dada
una palabra nos dirá su representación como "indice" en el vocabulario,
y el segundo que irá en dirección contraria.</p>
<p>A estos les llamaremos <code>word_to_idx</code> y
<code>idx_to_word</code>. La variable <code>vocab_size</code> nos dirá
el máximo de tamaño de nuestro vocabulario. Si intentamos acceder a una
palabra que no está en nuestro vocabulario, entonces se le reemplazará
con el token "UNK" o su índice correspondiente.</p>
</div>
<div id="8940eccd" class="cell code" data-execution_count="8"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.584569Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.568947Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6f52a0cd85402df075f20a68ae5f4e35&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5276b445f04c739b&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> seqs_to_dicts(sequences):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Crea word_to_idx y idx_to_word para una lista de secuencias</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    sequences: lista de secuencias a usar</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Diccionario de palabra a indice</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Diccionario de indice a palabra</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Int numero de secuencias</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Int tamaño del vocabulario</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lambda para aplanar (flatten) una lista de listas</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    flatten <span class="op">=</span> <span class="kw">lambda</span> l: [item <span class="cf">for</span> sublist <span class="kw">in</span> l <span class="cf">for</span> item <span class="kw">in</span> sublist]</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aplanamos el dataset</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    all_words <span class="op">=</span> flatten(sequences)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conteo de las ocurrencias de las palabras</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    word_count <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> all_words:</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        word_count[word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ordenar por frecuencia</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    word_count <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(word_count.items()), key<span class="op">=</span><span class="kw">lambda</span> x: <span class="op">-</span>x[<span class="dv">1</span>])</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear una lista de todas las palabras únicas</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    unique_words <span class="op">=</span> [w[<span class="dv">0</span>] <span class="cf">for</span> w <span class="kw">in</span> word_count]</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Agregamos UNK a la lista de palabras</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    unique_words.append(<span class="st">&quot;UNK&quot;</span>)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conteo del número de secuencias y el número de palabras unicas</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    num_sentences, vocab_size <span class="op">=</span> <span class="bu">len</span>(sequences), <span class="bu">len</span>(unique_words)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear diccionarios mencionados</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    word_to_idx <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: vocab_size<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    idx_to_word <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: <span class="st">&#39;UNK&#39;</span>)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Llenado de diccionarios</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, word <span class="kw">in</span> <span class="bu">enumerate</span>(unique_words):</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para agregar</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># word_to_idx[word] = </span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># idx_to_word[idx] = </span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        word_to_idx[word] <span class="op">=</span> idx</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        idx_to_word[idx] <span class="op">=</span> word</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> word_to_idx, idx_to_word, num_sentences, vocab_size</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>word_to_idx, idx_to_word, num_sequences, vocab_size <span class="op">=</span> seqs_to_dicts(sequences)</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Tenemos </span><span class="sc">{</span>num_sequences<span class="sc">}</span><span class="ss"> secuencias y </span><span class="sc">{</span><span class="bu">len</span>(word_to_idx)<span class="sc">}</span><span class="ss"> tokens unicos incluyendo UNK&quot;</span>)</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;El indice de &#39;b&#39; es </span><span class="sc">{</span>word_to_idx[<span class="st">&#39;b&#39;</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;La palabra con indice 1 es </span><span class="sc">{</span>idx_to_word[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Tenemos 100 secuencias y 4 tokens unicos incluyendo UNK
El indice de &#39;b&#39; es 1
La palabra con indice 1 es b
</code></pre>
</div>
</div>
<div id="8e17b1e6" class="cell code" data-execution_count="9"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:14.616010Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.584569Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e23613d7a17abd6db68772917d07f26d&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-c7aed80352919e68&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(word_to_idx), <span class="st">&#39;0xc51b9ba8&#39;</span>))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">2</span>):        </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(idx_to_word), <span class="st">&#39;0xc51b9ba8&#39;</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_string(idx_to_word[<span class="dv">0</span>], <span class="st">&#39;0xe8b7be43&#39;</span>))</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"2"}--> 
         ✓ [2 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="97cf7418" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;41c936e1a2f35b960bd2e805e9634b6a&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-650d92ab739231c8&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h3 id="representación-de-tokens-como-índices">Representación de tokens
como índices</h3>
<p>Como bien sabemos, necesitamos crear nuestro dataset de forma que el
se divida en inputs y targets para cada secuencia y luego particionar
esto en training, validation y test (80%, 10%, 10%). Debido a que
estamso haciendo prediccion de la siguiente palabra, nuestro target es
el input movido (shifted) una palabra.</p>
<p>Vamos a usar PyTorch solo para crear el dataset (como lo hicimos con
las imagenes de perritos y gatitos de los laboratorios pasados). Aunque
esta vez no haremos el dataloader. Recuerden que siempre es buena idea
usar un DataLoader para obtener los datos de una forma eficienciente, al
ser este un generador/iterador. Además, este nos sirve para obtener la
información en batches.</p>
</div>
<div id="f2d35905" class="cell code" data-execution_count="10"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.568534Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:14.616010Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;9e41ed4ad2165904a221567eab31e222&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-186baacdbd91cc05&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils <span class="im">import</span> data</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dataset(data.Dataset):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, targets):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> targets</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the size of the dataset</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.targets)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Retrieve inputs and targets at the given index</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> <span class="va">self</span>.inputs[index]</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.targets[index]</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X, y</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_datasets(sequences, dataset_class, p_train<span class="op">=</span><span class="fl">0.8</span>, p_val<span class="op">=</span><span class="fl">0.1</span>, p_test<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definimos el tamaño de las particiones</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    num_train <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(sequences)<span class="op">*</span>p_train)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    num_val <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(sequences)<span class="op">*</span>p_val)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    num_test <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(sequences)<span class="op">*</span>p_test)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dividir las secuencias en las particiones</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    sequences_train <span class="op">=</span> sequences[:num_train]</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    sequences_val <span class="op">=</span> sequences[num_train:num_train<span class="op">+</span>num_val]</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    sequences_test <span class="op">=</span> sequences[<span class="op">-</span>num_test:]</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Funcion interna para obtener los targets de una secuencia</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_inputs_targets_from_sequences(sequences):</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Listas vacias</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        inputs, targets <span class="op">=</span> [], []</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Agregar informacion a las listas, ambas listas tienen L-1 palabras de una secuencia de largo L</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pero los targetes están movidos a la derecha por uno, para que podamos predecir la siguiente palabra</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sequence <span class="kw">in</span> sequences:</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>            inputs.append(sequence[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>            targets.append(sequence[<span class="dv">1</span>:])</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> inputs, targets</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener inputs y targes para cada subgrupo</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    inputs_train, targets_train <span class="op">=</span> get_inputs_targets_from_sequences(sequences_train)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    inputs_val, targets_val <span class="op">=</span> get_inputs_targets_from_sequences(sequences_val)</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    inputs_test, targets_test <span class="op">=</span> get_inputs_targets_from_sequences(sequences_test)</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creación de datasets</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    training_set <span class="op">=</span> dataset_class(inputs_train, targets_train)</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>    validation_set <span class="op">=</span> dataset_class(inputs_val, targets_val)</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    test_set <span class="op">=</span> dataset_class(inputs_test, targets_test)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> training_set, validation_set, test_set</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>training_set, validation_set, test_set <span class="op">=</span> create_datasets(sequences, Dataset)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Largo del training set </span><span class="sc">{</span><span class="bu">len</span>(training_set)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Largo del validation set </span><span class="sc">{</span><span class="bu">len</span>(validation_set)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Largo del test set </span><span class="sc">{</span><span class="bu">len</span>(test_set)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Largo del training set 80
Largo del validation set 10
Largo del test set 10
</code></pre>
</div>
</div>
<div id="7ecf12af" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;68e5ac2cd049c56737d20e23c06b751e&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-f048a8b17dec6268&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h3 id="one-hot-encodings">One-Hot Encodings</h3>
<p>Ahora creemos una función simple para obtener la representación
one-hot encoding de dado un índice de una palabra. Noten que el tamaño
del one-hot encoding es igual a la del vocabulario. Adicionalmente
definamos una función para encodear una secuencia.</p>
</div>
<div id="a50596f4" class="cell code" data-execution_count="11"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.583462Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.570496Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6eea35ea244f238189afef746c0c3067&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-91e0dff1547fcd06&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode(idx, vocab_size):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Encodea una sola palabra dado su indice y el tamaño del vocabulario</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">     idx: indice de la palabra </span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: tamaño del vocabulario</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">    np.array de lagro &quot;vocab_size&quot;</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Init array encodeado</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    one_hot <span class="op">=</span> np.zeros(vocab_size)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setamos el elemento a uno</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    one_hot[idx] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> one_hot</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode_sequence(sequence, vocab_size):</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Encodea una secuencia de palabras dado el tamaño del vocabulario</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co">     sentence: una lista de palabras a encodear</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: tamaño del vocabulario</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="co">     </span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="co">    np.array 3D de tamaño (numero de palabras, vocab_size, 1)</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encodear cada palabra en la secuencia</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    encoding <span class="op">=</span> np.array([one_hot_encode(word_to_idx[word], vocab_size) <span class="cf">for</span> word <span class="kw">in</span> sequence])</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cambiar de forma para tener (num words, vocab size, 1)</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    encoding <span class="op">=</span> encoding.reshape(encoding.shape[<span class="dv">0</span>], encoding.shape[<span class="dv">1</span>], <span class="dv">1</span>)</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoding</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>test_word <span class="op">=</span> one_hot_encode(word_to_idx[<span class="st">&#39;a&#39;</span>], vocab_size)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Encodeado de &#39;a&#39; con forma </span><span class="sc">{</span>test_word<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>test_sentence <span class="op">=</span> one_hot_encode_sequence([<span class="st">&#39;a&#39;</span>, <span class="st">&#39;b&#39;</span>], vocab_size)</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Encodeado de la secuencia &#39;a b&#39; con forma </span><span class="sc">{</span>test_sentence<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">.&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Encodeado de &#39;a&#39; con forma (4,)
Encodeado de la secuencia &#39;a b&#39; con forma (2, 4, 1).
</code></pre>
</div>
</div>
<div id="16d828d1" class="cell markdown"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-07-30T04:31:37.634951Z&quot;,&quot;start_time&quot;:&quot;2023-07-30T04:31:37.621658Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;067f22f946e3fb34f11e220e7e8a387b&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-93bcd8db4fe6903f&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<p>Ahora que ya tenemos lo necesario de data para empezar a trabajar,
demos paso a hablar un poco más de las RNN</p>
<h2 id="redes-neuronales-recurrentes-rnn">Redes Neuronales Recurrentes
(RNN)</h2>
<p>Una red neuronal recurrente (RNN) es una red neuronal conocida por
modelar de manera efectiva datos secuenciales como el lenguaje, el habla
y las secuencias de proteínas. Procesa datos de manera cíclica,
aplicando los mismos cálculos a cada elemento de una secuencia. Este
enfoque cíclico permite que la red utilice cálculos anteriores como una
forma de memoria, lo que ayuda a hacer predicciones para cálculos
futuros. Para comprender mejor este concepto, consideren la siguiente
imagen.</p>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230518134831/What-is-Recurrent-Neural-Network.webp" alt="RNN" /></p>
<p><em>Crédito de imagen al autor, imagen tomada de "Introduction to
Recurrent Neural Network" de Aishwarya.27</em></p>
<p>Donde:</p>
<ul>
<li><span class="math inline"><em>x</em></span> es la secuencia de
input</li>
<li><span class="math inline"><em>U</em></span> es una matriz de pesos
aplicada a una muestra de input dada</li>
<li><span class="math inline"><em>V</em></span> es una matriz de pesos
usada para la computación recurrente para pasar la memroia en las
secuencias</li>
<li><span class="math inline"><em>W</em></span> es una matriz de pesos
usada para calcular la salida de cada paso</li>
<li><span class="math inline"><em>h</em></span> es el estado oculto
(hidden state) (memoria de la red) para cada paso</li>
<li><span class="math inline"><em>L</em></span> es la salida
resultante</li>
</ul>
<p>Cuando una red es extendida como se muestra, es más facil referirse a
un paso <span class="math inline"><em>t</em></span>. Tenemos los
siguientes calculos en la red</p>
<ul>
<li><span
class="math inline"><em>h</em><sub><em>t</em></sub> = <em>f</em>(<em>U</em><em>x</em><sub><em>t</em></sub> + <em>V</em><em>h</em><sub><em>t</em> − 1</sub></span>
donde f es la función de activacion</li>
<li><span
class="math inline"><em>L</em><sub><em>t</em></sub> = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em>(<em>W</em><em>h</em><sub><em>t</em></sub>)</span></li>
</ul>
<h3 id="implementando-una-rnn">Implementando una RNN</h3>
<p>Ahora pasaremos a inicializar nuestra RNN. Los pesos suelen
inicializar de forma aleatoria, pero esta vez lo haremos de forma
ortogonal para mejorar el rendimiento de nuestra red, y siguiendo las
recomendaciones del paper dado abajo.</p>
<p>Tenga cuidado al definir los elementos que se le piden, debido a que
una mala dimensión causará que tenga resultados diferentes y errores al
operar.</p>
</div>
<div id="e83d6394" class="cell code" data-execution_count="12"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.599419Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.584459Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7aab983af86e5257de37bcca64632cee&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-8c9797de901a1f19&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">50</span> <span class="co"># Numero de dimensiones en el hidden state</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>vocab_size  <span class="op">=</span> <span class="bu">len</span>(word_to_idx) <span class="co"># Tamaño del vocabulario</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_orthogonal(param):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Initializes weight parameters orthogonally.</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Inicializa los pesos ortogonalmente</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Esta inicialización está dada por el siguiente paper:</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co">    https://arxiv.org/abs/1312.6120</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.ndim <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&quot;Only parameters with 2 or more dimensions are supported.&quot;</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    rows, cols <span class="op">=</span> param.shape</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    new_param <span class="op">=</span> np.random.randn(rows, cols)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rows <span class="op">&lt;</span> cols:</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>        new_param <span class="op">=</span> new_param.T</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular factorización QR</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    q, r <span class="op">=</span> np.linalg.qr(new_param)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hacer Q uniforme de acuerdo a https://arxiv.org/pdf/math-ph/0609050.pdf</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> np.diag(r, <span class="dv">0</span>)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    ph <span class="op">=</span> np.sign(d)</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    q <span class="op">*=</span> ph</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rows <span class="op">&lt;</span> cols:</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.T</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    new_param <span class="op">=</span> q</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_param</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_rnn(hidden_size, vocab_size):</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a><span class="co">    Inicializa la RNN</span></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_size:  Dimensiones del hidden state</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: Dimensión del vocabulario</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definir la matriz de pesos (input del hidden state)</span></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> np.zeros((hidden_size, vocab_size))</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definir la matriz de pesos de los calculos recurrentes</span></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((hidden_size, hidden_size))</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definir la matriz de pesos del hidden state a la salida</span></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.zeros((vocab_size, hidden_size))</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias del hidden state</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>    b_hidden <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias de la salida</span></span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>    b_out <span class="op">=</span> np.zeros((vocab_size, <span class="dv">1</span>))</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 3 lineas para inicializar los pesos de forma ortogonal usando la</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># funcion init_orthogonal</span></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> init_orthogonal(U)</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> init_orthogonal(V)</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> init_orthogonal(W)</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> U, V, W, b_hidden, b_out</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_rnn(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size)</span></code></pre></div>
</div>
<div id="869c3065" class="cell code" data-execution_count="13"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.630336Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.603408Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5e16945840146775df25b57cf819b925&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-cebf0e26f26abbf2&quot;,&quot;locked&quot;:true,&quot;points&quot;:20,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">0</span>], ((<span class="dv">50</span>, <span class="dv">4</span>), <span class="fl">80.24369675632171</span>))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">1</span>], ((<span class="dv">50</span>, <span class="dv">50</span>), <span class="fl">3333.838548574836</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">2</span>], ((<span class="dv">4</span>, <span class="dv">50</span>), <span class="op">-</span><span class="fl">80.6410290517092</span>))</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">3</span>], ((<span class="dv">50</span>, <span class="dv">1</span>), <span class="fl">0.0</span>))</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">4</span>], ((<span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">0.0</span>))</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="4f6794e3" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;3824d97af49f2479f2f568049ce82d01&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-0af0cee7ee982788&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="funciones-de-activación">Funciones de Activación</h4>
<p>A continuación definiremos las funciones de activación a usar,
sigmoide, tanh y softmax.</p>
</div>
<div id="dd9c9363" class="cell code" data-execution_count="14"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.646294Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.631333Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a8ce75b321c0cc6ca5c2e37786a296f6&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-cda959974e86198a&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x, derivative<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula la función sigmoide para un array x</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">     x: El array sobre el que trabajar</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">     derivative: Si esta como verdadero, regresar el valor en la derivada</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    x_safe <span class="op">=</span> x <span class="op">+</span> <span class="fl">1e-12</span> <span class="co">#Evitar ceros</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x_safe))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa la derivada de la funcion</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> derivative: </span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> f)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa el valor para el paso forward</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x, derivative<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula la función tanh para un array x</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a><span class="co">     x: El array sobre el que trabajar</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a><span class="co">     derivative: Si esta como verdadero, regresar el valor en la derivada</span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>    x_safe <span class="op">=</span> x <span class="op">+</span> <span class="fl">1e-12</span> <span class="co">#Evitar ceros</span></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.tanh(x_safe)</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa la derivada de la funcion</span></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> derivative: </span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> f<span class="op">**</span><span class="dv">2</span></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa el valor para el paso forward</span></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x, derivative<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula la función softmax para un array x</span></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a><span class="co">     x: El array sobre el que trabajar</span></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a><span class="co">     derivative: Si esta como verdadero, regresar el valor en la derivada</span></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>    x_safe <span class="op">=</span> x <span class="op">+</span> <span class="fl">1e-12</span> <span class="co">#Evitar ceros</span></span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>    e_x <span class="op">=</span> np.exp(x_safe <span class="op">-</span> np.<span class="bu">max</span>(x_safe)) <span class="co"># Prevents numerical instability</span></span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> e_x <span class="op">/</span> e_x.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa la derivada de la funcion</span></span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> derivative: </span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span> <span class="co"># No se necesita en backpropagation</span></span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa el valor para el paso forward</span></span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f</span></code></pre></div>
</div>
<div id="86e6f5b7" class="cell code" data-execution_count="15"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.662250Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.647291Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;88b15c243905bba412ed5b4ba65b5be0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-a2ca064c7c460245&quot;,&quot;locked&quot;:true,&quot;points&quot;:15,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(sigmoid(params[<span class="dv">0</span>][<span class="dv">0</span>]), ((<span class="dv">4</span>,), <span class="fl">6.997641543410888</span>))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(tanh(params[<span class="dv">0</span>][<span class="dv">0</span>]), ((<span class="dv">4</span>,), <span class="op">-</span><span class="fl">0.007401604025076086</span>))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(softmax(params[<span class="dv">0</span>][<span class="dv">0</span>]), ((<span class="dv">4</span>,), <span class="fl">3.504688021096135</span>))</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="93ef9853" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;d75af82e072ddb4a0c162e849158bcc1&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-f6476b1310ebea2a&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="implementación-del-paso-forward">Implementación del paso
Forward</h4>
<p>Ahora es el momento de implementar el paso forward usando lo que
hemos implementado hasta ahora</p>
</div>
<div id="9bb06776" class="cell code" data-execution_count="16"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.677410Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.663247Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;65fdf4e2be5d9227b721ebfba3a76b88&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-d8f4885a4cccd525&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass(inputs, hidden_state, params):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula el paso forward de RNN</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">     inputs: Seccuencia de input a ser procesada</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_state: Un estado inicializado hidden state</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co">     params: Parametros de la RNN</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener los parametros</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    U, V, W, b_hidden, b_out <span class="op">=</span> params</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear una lista para guardar las salidas y los hidden states</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    outputs, hidden_states <span class="op">=</span> [], []</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada elemento en la secuencia input</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(inputs)):</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del nuevo hidden state usando tanh</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> tanh(U <span class="op">@</span> inputs[t] <span class="op">+</span> V <span class="op">@</span> hidden_state <span class="op">+</span> b_hidden)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Para el calculo del output</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Al ser la salida, deben usar softmax sobre la multiplicación de pesos de salida con el hidden_state actual</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> softmax(W <span class="op">@</span> hidden_state <span class="op">+</span> b_out)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Guardamos los resultados y continuamos</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>        outputs.append(out)</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        hidden_states.append(hidden_state.copy())</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs, hidden_states</span></code></pre></div>
</div>
<div id="1c095221" class="cell code" data-execution_count="17"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.693405Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.678377Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;b6b30539fff48162b40bf58b4d04a611&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-9db576244efaba24&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>test_input_sequence, test_target_sequence <span class="op">=</span> training_set[<span class="dv">0</span>]</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode </span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> one_hot_encode_sequence(test_input_sequence, vocab_size)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>test_target <span class="op">=</span> one_hot_encode_sequence(test_target_sequence, vocab_size)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state con zeros</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>outputs, hidden_states <span class="op">=</span> forward_pass(test_input, hidden_state, params)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Input:&quot;</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_input_sequence)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Target:&quot;</span>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_target_sequence)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Predicha:&quot;</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(outputs, ((<span class="dv">16</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">519.7419046193046</span>))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Secuencia Input:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
Secuencia Target:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
Secuencia Predicha:
[&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
</code></pre>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="ef14fc0c" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;c29e3a40c409913f6d3d0506d1b9d69f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-8419bbbbfb1d7d89&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="implementación-del-paso-backward">Implementación del paso
Backward</h4>
<p>Ahora es momento de implementar el paso backward. Si se pierden,
remitanse a las ecuaciones e imagen dadas previamente.</p>
<p>Usaremos una función auxiliar para evitar la explición del gradiente.
Esta tecnica suele funcionar muy bien, si quieren leer más sobre esto
pueden consultar estos enlances</p>
<p><a
href="https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem">Understanding
Gradient Clipping (and How It Can Fix Exploding Gradients
Problem)</a></p>
<p><a
href="https://ai.stackexchange.com/questions/31991/what-exactly-happens-in-gradient-clipping-by-norm">What
exactly happens in gradient clipping by norm?</a></p>
</div>
<div id="09c404d5" class="cell code" data-execution_count="18"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.709427Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.694424Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7357465e411ae111b649d95e4fd7d6eb&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-9c36e2544990bfd5&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clip_gradient_norm(grads, max_norm<span class="op">=</span><span class="fl">0.25</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Clipea (recorta?) el gradiente para tener una norma máxima de `max_norm`</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Esto ayudará a prevenir el problema de la gradiente explosiva (BOOM!)</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span> </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setea el máximo de la norma para que sea flotante</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    max_norm <span class="op">=</span> <span class="bu">float</span>(max_norm)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    total_norm <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculamos la norma L2 al cuadrado para cada gradiente y agregamos estas a la norma total</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> grad <span class="kw">in</span> grads:</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        grad_norm <span class="op">=</span> np.<span class="bu">sum</span>(np.power(grad, <span class="dv">2</span>))</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        total_norm <span class="op">+=</span> grad_norm</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cuadrado de la normal total</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    total_norm <span class="op">=</span> np.sqrt(total_norm)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculamos el coeficiente de recorte</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    clip_coef <span class="op">=</span> max_norm <span class="op">/</span> (total_norm <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Si el total de la norma es más grande que el máximo permitido, se recorta la gradiente</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> clip_coef <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> grad <span class="kw">in</span> grads:</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">*=</span> clip_coef</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grads</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward_pass(inputs, outputs, hidden_states, targets, params):</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula el paso backward de la RNN</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="co">     inputs: secuencia de input</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a><span class="co">     outputs: secuencia de output del forward</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_states: secuencia de los hidden_state del forward</span></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a><span class="co">     targets: secuencia target</span></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="co">     params: parametros de la RNN</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener los parametros</span></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>    U, V, W, b_hidden, b_out <span class="op">=</span> params</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inicializamos las gradientes como cero</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>    d_U, d_V, d_W <span class="op">=</span> np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>    d_b_hidden, d_b_out <span class="op">=</span> np.zeros_like(b_hidden), np.zeros_like(b_out)</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Llevar el record de las derivadas de los hidden state y las perdidas (loss)</span></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>    d_h_next <span class="op">=</span> np.zeros_like(hidden_states[<span class="dv">0</span>])</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(outputs))):</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> <span class="op">-</span>np.mean(np.log(outputs[t]<span class="op">+</span><span class="fl">1e-12</span>) <span class="op">*</span> targets[t])</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>        d_o <span class="op">=</span> outputs[t].copy()</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>        d_o[np.argmax(targets[t])] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>        d_W <span class="op">+=</span> np.dot(d_o, hidden_states[t].T)</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a>        d_b_out <span class="op">+=</span> d_o</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>        d_h <span class="op">=</span> np.dot(W.T, d_o) <span class="op">+</span> d_h_next</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>        d_f <span class="op">=</span> tanh(hidden_states[t], derivative<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> d_h</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a>        d_b_hidden <span class="op">+=</span> d_f</span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>        d_U <span class="op">+=</span> np.dot(d_f, inputs[t].T)</span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>        d_V <span class="op">+=</span> np.dot(d_f, hidden_states[t<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a>        d_h_next <span class="op">=</span> np.dot(V.T, d_f)</span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Empaquetar las gradientes</span></span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> d_U, d_V, d_W, d_b_hidden, d_b_out</span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Corte de gradientes</span></span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> clip_gradient_norm(grads)</span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, grads</span></code></pre></div>
</div>
<div id="21b7b1d7" class="cell code" data-execution_count="19"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.724695Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.710439Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e114a2a7bf6752fd90bf75a740001356&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-65758aa67361b673&quot;,&quot;locked&quot;:true,&quot;points&quot;:20,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>loss, grads <span class="op">=</span> backward_pass(test_input, outputs, hidden_states, test_target, params)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_scalar(loss, <span class="st">&#39;0xf0c8ccc9&#39;</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(grads[<span class="dv">0</span>], ((<span class="dv">50</span>, <span class="dv">4</span>), <span class="op">-</span><span class="fl">16.16536590645467</span>))</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(grads[<span class="dv">1</span>], ((<span class="dv">50</span>, <span class="dv">50</span>), <span class="op">-</span><span class="fl">155.12594909703253</span>))</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(grads[<span class="dv">2</span>], ((<span class="dv">4</span>, <span class="dv">50</span>), <span class="fl">1.5957812992239038</span>))</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="481b9abc" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;853d497293018f4e60eeaf31fa548bfd&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-06bca206671d7909&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="optimización">Optimización</h4>
<p>Considerando que ya tenemos el paso forward y podemos calcular
gradientes con el backpropagation, ya podemos pasar a entrenar nuestra
red. Para esto necesitaremos un optimizador. Una forma común y sencilla
es implementar la gradiente descediente. Recuerden la regla de
optimizacion <span
class="math display"><em>θ</em> = <em>θ</em> − <em>α</em> * ∇<em>J</em>(<em>θ</em>)</span></p>
<ul>
<li><span class="math inline"><em>θ</em></span> son los parametros del
modelo</li>
<li><span class="math inline"><em>α</em></span> es el learning rate</li>
<li><span class="math inline">∇<em>J</em>(<em>θ</em>)</span> representa
la gradiente del costo J con respecto de los parametros</li>
</ul>
</div>
<div id="7d0649f4" class="cell code" data-execution_count="20"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:29:15.740315Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.726168Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a816758f7791729583e774286d7ab13f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-54add6e82ed32f01&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_parameters(params, grads, lr<span class="op">=</span><span class="fl">1e-3</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteramos sobre los parametros y las gradientes</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param, grad <span class="kw">in</span> <span class="bu">zip</span>(params, grads):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        param <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span></code></pre></div>
</div>
<div id="985f024f" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;09c0aab76534abb28f1e0fa5f0bbd13c&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-52ac5cccec0e2107&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="entrenamiento">Entrenamiento</h4>
<p>Debemos establecer un ciclo de entrenamiento completo que involucre
un paso forward, un paso backprop, un paso de optimización y validación.
Se espera que el proceso de training dure aproximadamente 5 minutos (o
menos), lo que le brinda la oportunidad de continuar leyendo mientras se
ejecuta😜</p>
<p>Noten que estaremos viendo la perdida en el de validación (no en el
de testing) esto se suele hacer para ir observando que tan bien va
comportandose el modelo en terminos de generalización. Muchas veces es
más recomendable ir viendo como evoluciona la métrica de desempeño
principal (accuracy, recall, etc).</p>
</div>
<div id="1db77ee0" class="cell code" data-execution_count="21"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.382237Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:29:15.741282Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e207987552b230e721859e0270e1ad61&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-e184f5f494d827a1&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parametro</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Se coloca como &quot;repsuesta&quot; para que la herramienta no modifique el numero de iteraciones que colocaron </span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">#raise NotImplementedError()</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Init una nueva RNN</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_rnn(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hiddent state con ceros</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Rastreo de perdida (loss) para training y validacion</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>training_loss, validation_loss <span class="op">=</span> [], []</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos para cada epoca</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perdidas en zero</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    epoch_training_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    epoch_validation_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el grupo de validación</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> validation_set:</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el input y el target</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Re-init el hidden state</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> np.zeros_like(hidden_state)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso forward </span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs, hidden_states =</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>        outputs, hidden_states <span class="op">=</span> forward_pass(inputs_one_hot, hidden_state, params)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso backward</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss, _ =</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>        loss, _ <span class="op">=</span> backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de perdida</span></span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>        epoch_validation_loss <span class="op">+=</span> loss</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each sentence in training set</span></span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> training_set:</span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el input y el target</span></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Re-init el hidden state</span></span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> np.zeros_like(hidden_state)</span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso forward </span></span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs, hidden_states = </span></span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>        outputs, hidden_states <span class="op">=</span> forward_pass(inputs_one_hot, hidden_state, params)</span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso backward</span></span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss, grads = </span></span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)</span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validar si la perdida es nan, llegamos al problema del vanishing gradient POOF! </span></span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.isnan(loss):</span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&quot;La gradiente se desvanecio... POOF!&quot;</span>)</span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de parámetros</span></span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> update_parameters(params, grads, lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-77"><a href="#cb34-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de perdida</span></span>
<span id="cb34-78"><a href="#cb34-78" aria-hidden="true" tabindex="-1"></a>        epoch_training_loss <span class="op">+=</span> loss</span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guardar la perdida para graficar</span></span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a>    training_loss.append(epoch_training_loss<span class="op">/</span><span class="bu">len</span>(training_set))</span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(epoch_validation_loss<span class="op">/</span><span class="bu">len</span>(validation_set))</span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada 100 epocas</span></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&#39;Epoca </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, training loss: </span><span class="sc">{</span>training_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, validation loss: </span><span class="sc">{</span>validation_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoca 0, training loss: 4.05046509496538, validation loss: 4.801971835967156
Epoca 100, training loss: 2.729834076574944, validation loss: 3.2320576163982673
Epoca 200, training loss: 2.109414655736732, validation loss: 2.4980526328844146
Epoca 300, training loss: 1.823574698141341, validation loss: 2.1986770709845316
Epoca 400, training loss: 1.6884087861997372, validation loss: 2.077078608023497
Epoca 500, training loss: 1.6129170568126512, validation loss: 2.0163543941716586
Epoca 600, training loss: 1.5624028954062004, validation loss: 1.9780311638492247
Epoca 700, training loss: 1.5235019197917083, validation loss: 1.949613046784337
Epoca 800, training loss: 1.489582803129218, validation loss: 1.9248315278145838
Epoca 900, training loss: 1.4558865884071521, validation loss: 1.8978220912154378
Epoca 1000, training loss: 1.4173709332614932, validation loss: 1.860079817655525
Epoca 1100, training loss: 1.3681783634403957, validation loss: 1.7993697026414015
Epoca 1200, training loss: 1.3051122158818906, validation loss: 1.7081695076503602
Epoca 1300, training loss: 1.2330985128125058, validation loss: 1.5999314734390115
Epoca 1400, training loss: 1.1619900522538622, validation loss: 1.499857760238676
Epoca 1500, training loss: 1.1035554777966472, validation loss: 1.4282638416110474
Epoca 1600, training loss: 1.0680633416284258, validation loss: 1.395874591587123
Epoca 1700, training loss: 1.0550402179563676, validation loss: 1.3963674481755979
Epoca 1800, training loss: 1.0570111001893752, validation loss: 1.41857604438519
Epoca 1900, training loss: 1.064088062357339, validation loss: 1.4524183517051152
</code></pre>
</div>
</div>
<div id="37307e6b" class="cell code" data-execution_count="22"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.538285Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.382237Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;9c657c86ccab3ced18f8a9604bade0e2&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-67387da31438dd57&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Veamos la primera secuencia en el test set</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode el input y el target</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init el hidden state con ceros</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Hacemos el pase forward para evalular nuestra secuencia</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>outputs, hidden_states <span class="op">=</span> forward_pass(inputs_one_hot, hidden_state, params)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>output_sentence <span class="op">=</span> [idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs]</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Input:&quot;</span>)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Target:&quot;</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Predicha:&quot;</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficamos la perdida</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(training_loss))</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, training_loss, <span class="st">&#39;r&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training loss&#39;</span>,)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, validation_loss, <span class="st">&#39;b&#39;</span>, label<span class="op">=</span><span class="st">&#39;Validation loss&#39;</span>)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>), plt.ylabel(<span class="st">&#39;NLL&#39;</span>)</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">10</span>):        </span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_lists_by_percentage(targets, [idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs], <span class="dv">65</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Secuencia Input:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
Secuencia Target:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
Secuencia Predicha:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;, &#39;EOS&#39;]
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_c857c590cd71462387f9b785ba6f5f7d/08172efb625e4390e8d2e525e1b6ce1f689bde80.png" /></p>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"10"}--> 
         ✓ [10 marks] 
         </h1> </div>
</div>
</div>
<div id="8d3c223d" class="cell markdown"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-07-30T07:01:10.253203Z&quot;,&quot;start_time&quot;:&quot;2023-07-30T07:01:10.243032Z&quot;}">
<h4 id="preguntas">Preguntas</h4>
<p>Ya hemos visto el funcionamiento general de nuestra red RNN, viendo
las gráficas de arriba, <strong>responda</strong> lo siguiente dentro de
esta celda</p>
<ul>
<li>¿Qué interpretación le da a la separación de las graficas de
training y validation?</li>
</ul>
<p>La distancia entre las gráficas de training y validation brindan un
entendimiento del comportamiento de este modelo. Una amplia separación
entre estas curvas sugiere la posibilidad de que nuestro modelo, ha
memorizado las peculiaridades de los datos de entrenamiento, olvidando
la esencia subyacente. En términos más coloquiales, podría estar
haciendo overfitting. Por otro lado, si las curvas se acercan, tenemos
un indicativo de que nuestro modelo no solo ha comprendido los patrones
en el conjunto de entrenamiento, sino que también está preparado para
enfrentarse con gracia a datos desconocidos.</p>
<ul>
<li>¿Cree que es un buen modelo basado solamente en el loss?</li>
</ul>
<p>Confundir la métrica de pérdida (o 'loss') como la vara única para
medir la calidad de un modelo es un error tan común como juzgar un libro
únicamente por su cubierta. Sin embargo, el loss nos da una estimación
del error y es un indicativo de cómo el modelo está comprendiendo los
datos, pero no es el "oráculo" definitivo del desempeño. Es esencial que
no olvidemos considerar otras métricas y, muy importante, exponer el
modelo a situaciones inexploradas para realmente comprender su
capacidad.</p>
<ul>
<li>¿Cómo deberían de verse esas gráficas en un modelo ideal?</li>
</ul>
<p>En un mundo, donde código y datos se fusionan perfectamente, las
curvas de entrenamiento y validación descenderían juntas, en paralelo,
hacia los valores bajos. Mstrando que nuestro modelo no solo ha
capturado la esencia de los datos de entrenamiento, sino que también
está listo para generalizar y aplicar ese conocimiento a escenarios
desconocidos. El overfitting sería una rareza, pues el modelo se
centraría en aprender la melodía subyacente, en lugar de las
peculiaridades de cada nota.</p>
</div>
<div id="eda0001d" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;33717eb5a11832cbcf3afe049aa819f2&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-3b641dbd0cd4a7fa&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h2 id="parte-2---construyendo-una-red-neuronal-lstm">Parte 2 -
Construyendo una Red Neuronal LSTM</h2>
<p><strong>Créditos:</strong> La segunda parte de este laboratorio está
tomado y basado en uno de los laboratorios dados dentro del curso de
"Deep Learning" de Jes Frellsen (DeepLearningDTU)</p>
<p>Consideren leer el siguiente blog para mejorar el entendimiento de
este tema: <a
href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>La RNN estándar enfrenta un problema de gradientes que desaparecen,
lo que dificulta la retención de memoria en secuencias más largas. Para
hacer frente a estos desafíos, se introdujeron algunas variantes.</p>
<p>Los dos tipos principales son la celda de memoria a corto plazo
(LSTM) y la unidad recurrente cerrada (GRU), las cuales demuestran una
capacidad mejorada para conservar y utilizar la memoria en pasos de
tiempo posteriores.</p>
<p>En este ejercicio, nuestro enfoque estará en LSTM, pero los
principios aprendidos aquí también se pueden aplicar fácilmente para
implementar GRU.</p>
<p>Recordemos una de las imagenes que vimos en clase</p>
<p><img src="https://www.researchgate.net/profile/Savvas-Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg" alt="LSTM" /></p>
<p><em>Crédito de imagen al autor, imagen tomada de "Designing neural
network based decoders for surface codes" de Savvas
Varsamopoulos</em></p>
<p>Recordemos que la "celula" de LST contiene tres tipos de gates,
input, forget y output gate. La salida de una unidad LSTM está calculada
por las siguientes funciones, donde <span
class="math inline"><em>σ</em> = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em></span>.
Entonces tenemos la input gate <span
class="math inline"><em>i</em></span>, la forget gate <span
class="math inline"><em>f</em></span> y la output gate <span
class="math inline"><em>o</em></span></p>
<ul>
<li><span
class="math inline"><em>i</em> = <em>σ</em>(<em>W</em><sup><em>i</em></sup>[<em>h</em><sub><em>t</em> − 1</sub>,<em>x</em><sub><em>t</em></sub>])</span></li>
<li><span
class="math inline"><em>f</em> = <em>σ</em>(<em>W</em><sup><em>f</em></sup>[<em>h</em><sub><em>t</em> − 1</sub>,<em>x</em><sub><em>t</em></sub>])</span></li>
<li><span
class="math inline"><em>o</em> = <em>σ</em>(<em>W</em><sup><em>o</em></sup>[<em>h</em><sub><em>t</em> − 1</sub>,<em>x</em><sub><em>t</em></sub>])</span></li>
</ul>
<p>Donde <span
class="math inline"><em>W</em><sup><em>i</em></sup>, <em>W</em><sup><em>f</em></sup>, <em>W</em><sup><em>o</em></sup></span>
son las matrices de pesos aplicada a cada aplicadas a una matriz
contatenada <span
class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span> (hidden
state vector) y <span
class="math inline"><em>x</em><sub><em>t</em></sub></span> (input
vector) para cada respectiva gate <span
class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>, del paso
previo junto con el input actual <span
class="math inline"><em>x</em><sub><em>t</em></sub></span> son usados
para calcular una memoria candidata <span
class="math inline"><em>g</em></span></p>
<ul>
<li><span
class="math inline"><em>g</em> = <em>t</em><em>a</em><em>n</em><em>h</em>(<em>W</em><sup><em>g</em></sup>[<em>h</em><sub><em>t</em> − 1</sub>,<em>x</em><sub><em>t</em></sub>])</span></li>
</ul>
<p>El valor de la memoria <span
class="math inline"><em>c</em><sub><em>t</em></sub></span> es
actualizada como</p>
<p><span
class="math inline"><em>c</em><sub><em>t</em></sub> = <em>c</em><sub><em>t</em> − 1</sub> ∘ <em>f</em> + <em>g</em> ∘ <em>i</em></span></p>
<p>donde <span
class="math inline"><em>c</em><sub><em>t</em> − 1</sub></span> es la
memoria previa, y <span class="math inline">∘</span> es una
multiplicacion element-wise (recuerden que este tipo de multiplicación
en numpy es con *)</p>
<p>La salida <span
class="math inline"><em>h</em><sub><em>t</em></sub></span> es calculada
como</p>
<p><span
class="math inline"><em>h</em><sub><em>t</em></sub> = <em>t</em><em>a</em><em>n</em><em>h</em>(<em>c</em><sub><em>t</em></sub>) ∘ <em>o</em></span></p>
<p>y este se usa para tanto la salida del paso como para el siguiente
paso, mientras <span
class="math inline"><em>c</em><sub><em>t</em></sub></span> es
exclusivamente enviado al siguiente paso. Esto hace <span
class="math inline"><em>c</em><sub><em>t</em></sub></span> una memoria
feature, y no es usado directamente para caluclar la salida del paso
actual.</p>
<h3 id="iniciando-una-red-lstm">Iniciando una Red LSTM</h3>
<p>De forma similar a lo que hemos hecho antes, necesitaremos
implementar el paso forward, backward y un ciclo de entrenamiento. Pero
ahora usaremos LSTM con NumPy. Más adelante veremos como es que esto
funciona con PyTorch.</p>
</div>
<div id="a2c856b9" class="cell code" data-execution_count="23"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.553871Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.538285Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;62b5aae14a3dc0ee3dbca646ce607e19&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-07f509efcc1a3ccb&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Tamaño del hidden state concatenado más el input</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>z_size <span class="op">=</span> hidden_size <span class="op">+</span> vocab_size </span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_lstm(hidden_size, vocab_size, z_size):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Initializes our LSTM network.</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Init LSTM</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_size: Dimensiones del hidden state</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: Dimensiones de nuestro vocabulario</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="co">     z_size: Dimensiones del input concatenado </span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos de la forget gate</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recuerden que esta debe empezar con numeros aleatorios</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_f = np.random.randn</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    W_f <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">#raise NotImplementedError()</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias del forget gate</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>    b_f <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos de la input gate</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recuerden que esta debe empezar con numeros aleatorios</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    W_i <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#raise NotImplementedError()</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias para input gate</span></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    b_i <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos para la memoria candidata</span></span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recuerden que esta debe empezar con numeros aleatorios</span></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    W_g <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">#raise NotImplementedError()</span></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias para la memoria candidata</span></span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>    b_g <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos para la output gate</span></span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>    W_o <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias para la output gate</span></span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>    b_o <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz que relaciona el hidden state con el output</span></span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a>    W_v <span class="op">=</span> np.random.randn(vocab_size, hidden_size)</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias</span></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>    b_v <span class="op">=</span> np.zeros((vocab_size, <span class="dv">1</span>))</span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Init pesos ortogonalmente (https://arxiv.org/abs/1312.6120)</span></span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>    W_f <span class="op">=</span> init_orthogonal(W_f)</span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>    W_i <span class="op">=</span> init_orthogonal(W_i)</span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>    W_g <span class="op">=</span> init_orthogonal(W_g)</span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>    W_o <span class="op">=</span> init_orthogonal(W_o)</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a>    W_v <span class="op">=</span> init_orthogonal(W_v)</span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v</span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_lstm(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size, z_size<span class="op">=</span>z_size)</span></code></pre></div>
</div>
<div id="412a27b5" class="cell code" data-execution_count="24"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.569529Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.553871Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;f54f80a804b45836347ca5928b1902b0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-1145b5a61bdcda0f&quot;,&quot;locked&quot;:true,&quot;points&quot;:25,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">0</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="op">-</span><span class="fl">28071.583543573637</span>))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">1</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="op">-</span><span class="fl">6337.520066952928</span>))</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">2</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="op">-</span><span class="fl">13445.986473992281</span>))</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">3</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="fl">2276.1116210911564</span>))</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">4</span>], ((<span class="dv">4</span>, <span class="dv">50</span>), <span class="op">-</span><span class="fl">201.28961326044097</span>))</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="c8b5035e" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;7e55ee118bbd693b1c9f42414a5af868&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-c69b9a17df9ca940&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h3 id="forward">Forward</h3>
<p>Vamos para adelante con LSTM, al igual que previamente necesitamos
implementar las funciones antes mencionadas</p>
</div>
<div id="8a59a4dc" class="cell code" data-execution_count="25"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.585110Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.569529Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;b388082beee631c97ae27b131c638ee0&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1277d0634231924c&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(inputs, h_prev, C_prev, p):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co">    x: Input data en el paso &quot;t&quot;, shape (n_x, m)</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co">    h_prev: Hidden state en el paso &quot;t-1&quot;, shape (n_a, m)</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co">    C_prev: Memoria en el paso &quot;t-1&quot;, shape (n_a, m)</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co">    p: Lista con pesos y biases, contiene:</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_f:  Pesos de la forget gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_f: Bias de la forget gate, shape (n_a, 1)</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_i: Pesos de la update gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_i: Bias de la update gate, shape (n_a, 1)</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_g: Pesos de la primer &quot;tanh&quot;, shape (n_a, n_a + n_x)</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_g: Bias de la primer &quot;tanh&quot;, shape (n_a, 1)</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_o: Pesos de la output gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_o: Bias de la output gate, shape (n_a, 1)</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_v: Pesos de la matriz que relaciona el hidden state con el output, shape (n_v, n_a)</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_v: Bias que relaciona el hidden state con el output, shape (n_v, 1)</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="co">    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s: Lista de tamaño m conteniendo los calculos de cada paso forward</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a><span class="co">    outputs: Predicciones en el paso &quot;t&quot;, shape (n_v, m)</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validar las dimensiones</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> h_prev.shape <span class="op">==</span> (hidden_size, <span class="dv">1</span>)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> C_prev.shape <span class="op">==</span> (hidden_size, <span class="dv">1</span>)</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Desempacar los parametros</span></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v <span class="op">=</span> p</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Listas para calculos de cada componente en LSTM</span></span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>    x_s, z_s, f_s, i_s,  <span class="op">=</span> [], [] ,[], []</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>    g_s, C_s, o_s, h_s <span class="op">=</span> [], [] ,[], []</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>    v_s, output_s <span class="op">=</span>  [], [] </span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Agregar los valores iniciales </span></span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>    h_s.append(h_prev)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>    C_s.append(C_prev)</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> inputs:</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para concatenar el input y el hidden state</span></span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># z = np.row.stack(...)</span></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.row_stack((h_prev,x)) <span class="co"># YOUR CODE HERE</span></span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>        z_s.append(z)</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular el forget gate</span></span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: recuerde usar sigmoid</span></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># f = </span></span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>        f <span class="op">=</span> sigmoid(np.dot(W_f, z) <span class="op">+</span> b_f)<span class="co"># YOUR CODE HERE</span></span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>        f_s.append(f)</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del input gate</span></span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> sigmoid(np.dot(W_i, z) <span class="op">+</span> b_i)</span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>        i_s.append(i)</span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de la memoria candidata</span></span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> tanh(np.dot(W_g, z) <span class="op">+</span> b_g)</span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a>        g_s.append(g)</span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular el estado de la memoria</span></span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># C_prev = </span></span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a>        C_prev <span class="op">=</span> f <span class="op">*</span> C_prev <span class="op">+</span> i <span class="op">*</span> g<span class="co"># YOUR CODE HERE</span></span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb40-66"><a href="#cb40-66" aria-hidden="true" tabindex="-1"></a>        C_s.append(C_prev)</span>
<span id="cb40-67"><a href="#cb40-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-68"><a href="#cb40-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el calculo de la output gate</span></span>
<span id="cb40-69"><a href="#cb40-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: recuerde usar sigmoid</span></span>
<span id="cb40-70"><a href="#cb40-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># o = </span></span>
<span id="cb40-71"><a href="#cb40-71" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> sigmoid(np.dot(W_o, z) <span class="op">+</span> b_o)<span class="co"># YOUR CODE HERE</span></span>
<span id="cb40-72"><a href="#cb40-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb40-73"><a href="#cb40-73" aria-hidden="true" tabindex="-1"></a>        o_s.append(o)</span>
<span id="cb40-74"><a href="#cb40-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-75"><a href="#cb40-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate hidden state</span></span>
<span id="cb40-76"><a href="#cb40-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el calculo del hidden state</span></span>
<span id="cb40-77"><a href="#cb40-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h_prev =</span></span>
<span id="cb40-78"><a href="#cb40-78" aria-hidden="true" tabindex="-1"></a>        h_prev <span class="op">=</span> o <span class="op">*</span> tanh(C_prev)<span class="co"># YOUR CODE HERE</span></span>
<span id="cb40-79"><a href="#cb40-79" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb40-80"><a href="#cb40-80" aria-hidden="true" tabindex="-1"></a>        h_s.append(h_prev)</span>
<span id="cb40-81"><a href="#cb40-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-82"><a href="#cb40-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calcular logits</span></span>
<span id="cb40-83"><a href="#cb40-83" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.dot(W_v, h_prev) <span class="op">+</span> b_v</span>
<span id="cb40-84"><a href="#cb40-84" aria-hidden="true" tabindex="-1"></a>        v_s.append(v)</span>
<span id="cb40-85"><a href="#cb40-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-86"><a href="#cb40-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de output (con softmax)</span></span>
<span id="cb40-87"><a href="#cb40-87" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> softmax(v)</span>
<span id="cb40-88"><a href="#cb40-88" aria-hidden="true" tabindex="-1"></a>        output_s.append(output)</span>
<span id="cb40-89"><a href="#cb40-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-90"><a href="#cb40-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s</span></code></pre></div>
</div>
<div id="71f758df" class="cell code" data-execution_count="26"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.600776Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.585110Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;94b91568cf22e1f75709bfe774316fd7&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-4c878e36c9c270ab&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la primera secuencia para probar</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode del input y target</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state con ceros</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>output_sentence <span class="op">=</span> [idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs]</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Input:&quot;</span>)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Target:&quot;</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Predicha:&quot;</span>)</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(outputs, ((<span class="dv">22</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">980.1651308051631</span>))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Secuencia Input:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
Secuencia Target:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
Secuencia Predicha:
[&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
</code></pre>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="b6473816" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;a336c2502c28403858fffbc0ec095bb2&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-f1fb26540d33e61b&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h3 id="backward">Backward</h3>
<p>Ahora de reversa, al igual que lo hecho antes, necesitamos
implementar el paso de backward</p>
</div>
<div id="a753b92e" class="cell code" data-execution_count="27"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.616358Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.600776Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;149234786a31e8903430dfe2ff9b25aa&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-8500a307f5192db0&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(z, f, i, g, C, o, h, v, outputs, targets, p <span class="op">=</span> params):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co">    z: Input concatenado como una lista de tamaño m.</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co">    f: Calculos del forget gate como una lista de tamaño m.</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co">    i: Calculos del input gate como una lista de tamaño m.</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="co">    g: Calculos de la memoria candidata como una lista de tamaño m.</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co">    C: Celdas estado como una lista de tamaño m+1.</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co">    o: Calculos del output gate como una lista de tamaño m.</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="co">    h: Calculos del Hidden State como una lista de tamaño m+1.</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co">    v: Calculos del logit como una lista de tamaño m.</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="co">    outputs: Salidas como una lista de tamaño m.</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="co">    targets: Targets como una lista de tamaño m.</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co">    p: Lista con pesos y biases, contiene:</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_f:  Pesos de la forget gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_f: Bias de la forget gate, shape (n_a, 1)</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_i: Pesos de la update gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_i: Bias de la update gate, shape (n_a, 1)</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_g: Pesos de la primer &quot;tanh&quot;, shape (n_a, n_a + n_x)</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_g: Bias de la primer &quot;tanh&quot;, shape (n_a, 1)</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_o: Pesos de la output gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_o: Bias de la output gate, shape (n_a, 1)</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_v: Pesos de la matriz que relaciona el hidden state con el output, shape (n_v, n_a)</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_v: Bias que relaciona el hidden state con el output, shape (n_v, 1)</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a><span class="co">    loss: crossentropy loss para todos los elementos del output</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a><span class="co">    grads: lista de gradientes para todos los elementos en p</span></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Desempacar parametros</span></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v <span class="op">=</span> p</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Init gradientes con cero</span></span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    W_f_d <span class="op">=</span> np.zeros_like(W_f)</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>    b_f_d <span class="op">=</span> np.zeros_like(b_f)</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>    W_i_d <span class="op">=</span> np.zeros_like(W_i)</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>    b_i_d <span class="op">=</span> np.zeros_like(b_i)</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>    W_g_d <span class="op">=</span> np.zeros_like(W_g)</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>    b_g_d <span class="op">=</span> np.zeros_like(b_g)</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>    W_o_d <span class="op">=</span> np.zeros_like(W_o)</span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a>    b_o_d <span class="op">=</span> np.zeros_like(b_o)</span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>    W_v_d <span class="op">=</span> np.zeros_like(W_v)</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a>    b_v_d <span class="op">=</span> np.zeros_like(b_v)</span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setear la proxima unidad y hidden state con ceros</span></span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a>    dh_next <span class="op">=</span> np.zeros_like(h[<span class="dv">0</span>])</span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a>    dC_next <span class="op">=</span> np.zeros_like(C[<span class="dv">0</span>])</span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-53"><a href="#cb43-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para la perdida</span></span>
<span id="cb43-54"><a href="#cb43-54" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb43-55"><a href="#cb43-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-56"><a href="#cb43-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteramos en reversa los outputs</span></span>
<span id="cb43-57"><a href="#cb43-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(outputs))):</span>
<span id="cb43-58"><a href="#cb43-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-59"><a href="#cb43-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida con cross entropy</span></span>
<span id="cb43-60"><a href="#cb43-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss += ...</span></span>
<span id="cb43-61"><a href="#cb43-61" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> <span class="op">-</span>np.mean(np.log(outputs[t]) <span class="op">*</span> targets[t])<span class="co"># YOUR CODE HERE</span></span>
<span id="cb43-62"><a href="#cb43-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb43-63"><a href="#cb43-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-64"><a href="#cb43-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Obtener el hidden state del estado previo</span></span>
<span id="cb43-65"><a href="#cb43-65" aria-hidden="true" tabindex="-1"></a>        C_prev<span class="op">=</span> C[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb43-66"><a href="#cb43-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-67"><a href="#cb43-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivative of the relation of the hidden-state to the output gate</span></span>
<span id="cb43-68"><a href="#cb43-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas en relacion del hidden state al output gate</span></span>
<span id="cb43-69"><a href="#cb43-69" aria-hidden="true" tabindex="-1"></a>        dv <span class="op">=</span> np.copy(outputs[t])</span>
<span id="cb43-70"><a href="#cb43-70" aria-hidden="true" tabindex="-1"></a>        dv[np.argmax(targets[t])] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb43-71"><a href="#cb43-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-72"><a href="#cb43-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para actualizar la gradiente de la relacion del hidden-state al output gate</span></span>
<span id="cb43-73"><a href="#cb43-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># W_v_d += </span></span>
<span id="cb43-74"><a href="#cb43-74" aria-hidden="true" tabindex="-1"></a>        W_v_d <span class="op">+=</span> np.dot(dv, h[t].T)<span class="co"># YOUR CODE HERE</span></span>
<span id="cb43-75"><a href="#cb43-75" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb43-76"><a href="#cb43-76" aria-hidden="true" tabindex="-1"></a>        b_v_d <span class="op">+=</span> dv</span>
<span id="cb43-77"><a href="#cb43-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-78"><a href="#cb43-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de la derivada del hidden state y el output gate</span></span>
<span id="cb43-79"><a href="#cb43-79" aria-hidden="true" tabindex="-1"></a>        dh <span class="op">=</span> np.dot(W_v.T, dv)        </span>
<span id="cb43-80"><a href="#cb43-80" aria-hidden="true" tabindex="-1"></a>        dh <span class="op">+=</span> dh_next</span>
<span id="cb43-81"><a href="#cb43-81" aria-hidden="true" tabindex="-1"></a>        do <span class="op">=</span> dh <span class="op">*</span> tanh(C[t])</span>
<span id="cb43-82"><a href="#cb43-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la derivada del output</span></span>
<span id="cb43-83"><a href="#cb43-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># do = ..</span></span>
<span id="cb43-84"><a href="#cb43-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Recuerde multiplicar por el valor previo de do (el de arriba)</span></span>
<span id="cb43-85"><a href="#cb43-85" aria-hidden="true" tabindex="-1"></a>        do <span class="op">=</span> do <span class="op">*</span> o[t] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> o[t])<span class="co"># YOUR CODE HERE</span></span>
<span id="cb43-86"><a href="#cb43-86" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb43-87"><a href="#cb43-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-88"><a href="#cb43-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de las gradientes con respecto al output gate</span></span>
<span id="cb43-89"><a href="#cb43-89" aria-hidden="true" tabindex="-1"></a>        W_o_d <span class="op">+=</span> np.dot(do, z[t].T)</span>
<span id="cb43-90"><a href="#cb43-90" aria-hidden="true" tabindex="-1"></a>        b_o_d <span class="op">+=</span> do</span>
<span id="cb43-91"><a href="#cb43-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-92"><a href="#cb43-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas del estado y la memoria candidata g</span></span>
<span id="cb43-93"><a href="#cb43-93" aria-hidden="true" tabindex="-1"></a>        dC <span class="op">=</span> np.copy(dC_next)</span>
<span id="cb43-94"><a href="#cb43-94" aria-hidden="true" tabindex="-1"></a>        dC <span class="op">+=</span> dh <span class="op">*</span> o[t] <span class="op">*</span> tanh(tanh(C[t]), derivative<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-95"><a href="#cb43-95" aria-hidden="true" tabindex="-1"></a>        dg <span class="op">=</span> dC <span class="op">*</span> i[t]</span>
<span id="cb43-96"><a href="#cb43-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea de codigo para terminar el calculo de dg</span></span>
<span id="cb43-97"><a href="#cb43-97" aria-hidden="true" tabindex="-1"></a>        dg <span class="op">=</span> dg <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> g[t] <span class="op">**</span> <span class="dv">2</span>)<span class="co"># YOUR CODE HERE</span></span>
<span id="cb43-98"><a href="#cb43-98" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb43-99"><a href="#cb43-99" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-100"><a href="#cb43-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de las gradientes con respecto de la mem candidata</span></span>
<span id="cb43-101"><a href="#cb43-101" aria-hidden="true" tabindex="-1"></a>        W_g_d <span class="op">+=</span> np.dot(dg, z[t].T)</span>
<span id="cb43-102"><a href="#cb43-102" aria-hidden="true" tabindex="-1"></a>        b_g_d <span class="op">+=</span> dg</span>
<span id="cb43-103"><a href="#cb43-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-104"><a href="#cb43-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivative of the input gate and update its gradients</span></span>
<span id="cb43-105"><a href="#cb43-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de la derivada del input gate y la actualización de sus gradientes</span></span>
<span id="cb43-106"><a href="#cb43-106" aria-hidden="true" tabindex="-1"></a>        di <span class="op">=</span> dC <span class="op">*</span> g[t]</span>
<span id="cb43-107"><a href="#cb43-107" aria-hidden="true" tabindex="-1"></a>        di <span class="op">=</span> sigmoid(i[t], <span class="va">True</span>) <span class="op">*</span> di</span>
<span id="cb43-108"><a href="#cb43-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para el calculo de los pesos y bias del input gate</span></span>
<span id="cb43-109"><a href="#cb43-109" aria-hidden="true" tabindex="-1"></a>        W_i_d <span class="op">+=</span> np.dot(di, z[t].T) <span class="co"># W_i_d += </span></span>
<span id="cb43-110"><a href="#cb43-110" aria-hidden="true" tabindex="-1"></a>        b_i_d <span class="op">+=</span> di <span class="co"># b_i_d +=</span></span>
<span id="cb43-111"><a href="#cb43-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb43-112"><a href="#cb43-112" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb43-113"><a href="#cb43-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-114"><a href="#cb43-114" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas del forget gate y actualización de sus gradientes</span></span>
<span id="cb43-115"><a href="#cb43-115" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> dC <span class="op">*</span> C_prev</span>
<span id="cb43-116"><a href="#cb43-116" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> sigmoid(f[t]) <span class="op">*</span> df</span>
<span id="cb43-117"><a href="#cb43-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para el calculo de los pesos y bias de la forget gate</span></span>
<span id="cb43-118"><a href="#cb43-118" aria-hidden="true" tabindex="-1"></a>        W_f_d <span class="op">+=</span> np.dot(df, z[t].T) <span class="co"># W_f_d += </span></span>
<span id="cb43-119"><a href="#cb43-119" aria-hidden="true" tabindex="-1"></a>        b_f_d <span class="op">+=</span> df <span class="co"># b_f_d +=</span></span>
<span id="cb43-120"><a href="#cb43-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb43-121"><a href="#cb43-121" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb43-122"><a href="#cb43-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-123"><a href="#cb43-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas del input y la actualizacion de gradientes del hidden state previo</span></span>
<span id="cb43-124"><a href="#cb43-124" aria-hidden="true" tabindex="-1"></a>        dz <span class="op">=</span> (np.dot(W_f.T, df)</span>
<span id="cb43-125"><a href="#cb43-125" aria-hidden="true" tabindex="-1"></a>             <span class="op">+</span> np.dot(W_i.T, di)</span>
<span id="cb43-126"><a href="#cb43-126" aria-hidden="true" tabindex="-1"></a>             <span class="op">+</span> np.dot(W_g.T, dg)</span>
<span id="cb43-127"><a href="#cb43-127" aria-hidden="true" tabindex="-1"></a>             <span class="op">+</span> np.dot(W_o.T, do))</span>
<span id="cb43-128"><a href="#cb43-128" aria-hidden="true" tabindex="-1"></a>        dh_prev <span class="op">=</span> dz[:hidden_size, :]</span>
<span id="cb43-129"><a href="#cb43-129" aria-hidden="true" tabindex="-1"></a>        dC_prev <span class="op">=</span> f[t] <span class="op">*</span> dC</span>
<span id="cb43-130"><a href="#cb43-130" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-131"><a href="#cb43-131" aria-hidden="true" tabindex="-1"></a>    grads<span class="op">=</span> W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d</span>
<span id="cb43-132"><a href="#cb43-132" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-133"><a href="#cb43-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recorte de gradientes</span></span>
<span id="cb43-134"><a href="#cb43-134" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> clip_gradient_norm(grads)</span>
<span id="cb43-135"><a href="#cb43-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-136"><a href="#cb43-136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, grads</span></code></pre></div>
</div>
<div id="58def9bc" class="cell code" data-execution_count="28"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:32:05.631979Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.616358Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;f17904c9bbc54f6acdd9e59ead87adc0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-baf03f239d56e288&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Realizamos un backward pass para probar</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>loss, grads <span class="op">=</span> backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Perdida obtenida:</span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(loss, <span class="st">&#39;0x53c34f25&#39;</span>))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Perdida obtenida:7.637217940763248
</code></pre>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="8739dbcd" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;d33c26c1ed061d46ae3bb649a1d8f4e0&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-68df4c065c8367d9&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h3 id="training">Training</h3>
<p>Ahora intentemos entrenar nuestro LSTM básico. Esta parte es muy
similar a lo que ya hicimos previamente con la RNN</p>
</div>
<div id="c250482a" class="cell code" data-execution_count="29"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:34:07.095962Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:32:05.631979Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1882170a6b982a00cd873c6d50cc1e09&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-cf9622776d252627&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parametros</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Init una nueva red</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>z_size <span class="op">=</span> hidden_size <span class="op">+</span> vocab_size <span class="co"># Tamaño del hidden concatenado + el input</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_lstm(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size, z_size<span class="op">=</span>z_size)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state como ceros</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Perdida</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>training_loss, validation_loss <span class="op">=</span> [], []</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos cada epoca</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perdidas</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    epoch_training_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>    epoch_validation_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el validation set</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> validation_set:</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Init hidden state y la unidad de estado como ceros</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward </span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>        loss, _ <span class="op">=</span> backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>        epoch_validation_loss <span class="op">+=</span> loss</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el training set</span></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> training_set:</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Init hidden state y la unidad de estado como ceros</span></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward</span></span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward</span></span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de parametros</span></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> update_parameters(params, grads, lr<span class="op">=</span><span class="fl">1e-1</span>)</span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a>        epoch_training_loss <span class="op">+=</span> loss</span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guardar la perdida para ser graficada</span></span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>    training_loss.append(epoch_training_loss<span class="op">/</span><span class="bu">len</span>(training_set))</span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(epoch_validation_loss<span class="op">/</span><span class="bu">len</span>(validation_set))</span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada 5 epocas</span></span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&#39;Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, training loss: </span><span class="sc">{</span>training_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, validation loss: </span><span class="sc">{</span>validation_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 0, training loss: 2.988603772853675, validation loss: 4.499707061171418
Epoch 10, training loss: 1.2138346782420837, validation loss: 1.443017449173577
Epoch 20, training loss: 0.9205397027246413, validation loss: 1.089850853930022
Epoch 30, training loss: 0.9461880893217114, validation loss: 1.5162333116430573
Epoch 40, training loss: 0.8447097453708239, validation loss: 1.2637189800574518
Epoch 50, training loss: 0.8210537143874097, validation loss: 1.2153374399092132
Epoch 60, training loss: 0.8234789823165087, validation loss: 1.262060603937272
Epoch 70, training loss: 0.8193493781664719, validation loss: 1.2509081934594906
Epoch 80, training loss: 0.825162211305458, validation loss: 1.2643167682476868
Epoch 90, training loss: 0.8335299528338794, validation loss: 1.311726663041251
Epoch 100, training loss: 0.8452056669790936, validation loss: 1.346602278942647
Epoch 110, training loss: 0.8795634139668811, validation loss: 1.5241101691996608
Epoch 120, training loss: 0.9038403083892776, validation loss: 1.638356073613571
Epoch 130, training loss: 0.909552503439043, validation loss: 1.6592155285015693
Epoch 140, training loss: 0.900108174017553, validation loss: 1.6201020554944634
Epoch 150, training loss: 0.8771005280496279, validation loss: 1.5335961914126635
Epoch 160, training loss: 0.8373765510273241, validation loss: 1.3828966640202993
Epoch 170, training loss: 0.7731721265634427, validation loss: 1.1227207896063398
Epoch 180, training loss: 0.7266228290665203, validation loss: 0.8363381912487409
Epoch 190, training loss: 0.7630666507434529, validation loss: 0.904597018744143
Epoch 200, training loss: 0.8029831921164708, validation loss: 1.0697653390659076
Epoch 210, training loss: 0.8164207500870427, validation loss: 1.1478987519878623
Epoch 220, training loss: 0.8244645342521512, validation loss: 1.1993745369194524
Epoch 230, training loss: 0.8304826014661837, validation loss: 1.234657318722853
Epoch 240, training loss: 0.8252078588269504, validation loss: 1.2127946099788862
Epoch 250, training loss: 0.8067015254913514, validation loss: 1.1383059179072952
Epoch 260, training loss: 0.7781123570830506, validation loss: 1.0372930086607786
Epoch 270, training loss: 0.7461939570242271, validation loss: 0.9262763135396928
Epoch 280, training loss: 0.7187021141135842, validation loss: 0.8508436113147969
Epoch 290, training loss: 0.6997819849284078, validation loss: 0.8307801697088356
Epoch 300, training loss: 0.6907102716780675, validation loss: 0.8799567484797273
Epoch 310, training loss: 0.6877451077905203, validation loss: 0.9422632200439682
Epoch 320, training loss: 0.6889083658215852, validation loss: 0.9554895878817234
Epoch 330, training loss: 0.693447642038852, validation loss: 0.922634385839048
Epoch 340, training loss: 0.7019862410717361, validation loss: 0.8662937051050277
Epoch 350, training loss: 0.7126839521404047, validation loss: 0.8321971399701308
Epoch 360, training loss: 0.7858620441269694, validation loss: 0.849475717569258
Epoch 370, training loss: 1.0918171878664036, validation loss: 0.8829595678548301
Epoch 380, training loss: 2.4077346452513635, validation loss: 5.839149321690478
Epoch 390, training loss: 2.0775448816049567, validation loss: 4.370072562113064
Epoch 400, training loss: 0.8646637056092811, validation loss: 1.3785094125607047
Epoch 410, training loss: 0.7473683433698743, validation loss: 0.9021029248653468
Epoch 420, training loss: 0.7794666745794128, validation loss: 0.9199545600421114
Epoch 430, training loss: 0.7940313795381526, validation loss: 0.9099175189065454
Epoch 440, training loss: 0.8114174147364199, validation loss: 0.9039386008160957
Epoch 450, training loss: 0.8397053687697229, validation loss: 0.9224015505689047
Epoch 460, training loss: 0.8822009456520965, validation loss: 0.9680250473404859
Epoch 470, training loss: 0.9320670386445622, validation loss: 1.0327259928213954
Epoch 480, training loss: 0.9676229660412737, validation loss: 1.1501898224017304
Epoch 490, training loss: 0.989126307908189, validation loss: 1.5033645288897612
</code></pre>
</div>
</div>
<div id="531a11a0" class="cell code" data-execution_count="30"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:34:07.236710Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:34:07.095962Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5db6b37684f2913ca50ec8a4c8f5981f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-7814184dd4823fac&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la primera secuencia del test set</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode el input y el target</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state como ceros</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward </span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Input:&quot;</span>)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Target:&quot;</span>)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Predicha:&quot;</span>)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar la perdida en training y validacion</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(training_loss))</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, training_loss, <span class="st">&#39;r&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training loss&#39;</span>,)</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, validation_loss, <span class="st">&#39;b&#39;</span>, label<span class="op">=</span><span class="st">&#39;Validation loss&#39;</span>)</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>), plt.ylabel(<span class="st">&#39;NLL&#39;</span>)</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Secuencia Input:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
Secuencia Target:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
Secuencia Predicha:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;, &#39;EOS&#39;, &#39;EOS&#39;]
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_c857c590cd71462387f9b785ba6f5f7d/25d5fd9945570a76fe44099fd413c104039177e3.png" /></p>
</div>
</div>
<div id="c8692424" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;c4c179da7bfac5c35f0bc42867fe83cf&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-a8b4db0d7c0dd6cc&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="preguntas">Preguntas</h4>
<p><strong>Responda</strong> lo siguiente dentro de esta celda</p>
<ul>
<li>¿Qué modelo funcionó mejor? ¿RNN tradicional o el basado en LSTM?
¿Por qué?</li>
</ul>
<p>El modelo RNN tradicional ha demostrado ser superior en este
escenario particular. Una de las principales ventajas de un RNN sobre su
contraparte LSTM es su simplicidad inherente. Esta simplicidad no solo
reduce la carga computacional, sino que también minimiza el riesgo de
sobreajuste, especialmente cuando el conjunto de datos no es
extremadamente complejo o extenso.</p>
<ul>
<li>Observen la gráfica obtenida arriba, ¿en qué es diferente a la
obtenida a RNN? ¿Es esto mejor o peor? ¿Por qué?</li>
</ul>
<p>Al examinar las gráficas, se puede observar que el desempeño del LSTM
varía más a lo largo de las épocas en comparación con el RNN
tradicional. Mientras que el LSTM podría tener el potencial de superar
al RNN en ciertos escenarios, en esta instancia específica, la
consistencia y simplicidad del RNN lo convierten en la elección
óptima.</p>
<ul>
<li>¿Por qué LSTM puede funcionar mejor con secuencias largas?</li>
</ul>
<p>El diseño subyacente de los LSTM se creó para abordar las
deficiencias de los RNNs en el manejo de secuencias más largas, en
particular, el problema del desvanecimiento del gradiente. Los LSTM
cuentan con una estructura de puertas y celdas de memoria que les
permite decidir qué información retener y cuál olvidar en cada paso
temporal. Esta capacidad le da una ventaja significativa en tareas que
requieren considerar dependencias a largo plazo, como el procesamiento
del lenguaje natural o la predicción de series temporales. Sin embargo,
su eficacia dependerá en última instancia del contexto y de la
naturaleza de los datos con los que se esté trabajando.</p>
</div>
<div id="3bdc991e" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;f7b6dbdd11cf95d69a352306b879c05b&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-70b696ca36c0804a&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h2 id="parte-3---red-neuronal-lstm-con-pytorch">Parte 3 - Red Neuronal
LSTM con PyTorch</h2>
<p>Ahora que ya hemos visto el funcionamiento paso a paso de tanto RNN
tradicional como LSTM. Es momento de usar PyTorch. Para esta parte
usaremos el mismo dataset generado al inicio. Así mismo, usaremos un
ciclo de entrenamiento similar al que hemos usado previamente.</p>
<p>En la siguiente parte (sí, hay una siguiente parte 🤓) usaremos otro
tipo de dataset más formal</p>
</div>
<div id="0b18938b" class="cell code" data-execution_count="31"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:34:07.267949Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:34:07.236710Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;ee2d3fa1a4e9d2426203334a38a4af8e&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-311fc1fe42eca687&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1-3 lineas de codigo para declarar una capa LSTM</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.lstm = </span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Esta tiene que tener el input_size del tamaño del vocabulario,</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     debe tener 50 hidden states (hidden_size)</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     una layer</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     y NO (False) debe ser bidireccional </span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(input_size<span class="op">=</span>vocab_size,</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>                            hidden_size<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>                            num_layers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>                            bidirectional<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer de salida (output)</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l_out <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>                            out_features<span class="op">=</span>vocab_size,</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>                            bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RNN regresa el output y el ultimo hidden state</span></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>        x, (h, c) <span class="op">=</span> <span class="va">self</span>.lstm(x)</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aplanar la salida para una layer feed forward</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.lstm.hidden_size)</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layer de output </span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.l_out(x)</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(net)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Net(
  (lstm): LSTM(4, 50)
  (l_out): Linear(in_features=50, out_features=4, bias=False)
)
</code></pre>
</div>
</div>
<div id="8c896f71" class="cell code" data-execution_count="32"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:00.753913Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:34:07.267949Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;ad63c124dd865aa9b8c0da08852718ad&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-04486b8d9ade1533&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parametros</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Init una nueva red</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 2 lineas para definir la función de perdida y el optimizador</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = # Use CrossEntropy</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = # Use Adam con lr=3e-4</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(net.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perdida</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>training_loss, validation_loss <span class="op">=</span> [], []</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos cada epoca</span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perdidas</span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    epoch_training_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    epoch_validation_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NOTA 1</span></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>    net.<span class="bu">eval</span>()</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el validation set</span></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> validation_set:</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> [word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> targets]</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el input a un tensor</span></span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> torch.Tensor(inputs_one_hot)</span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> inputs_one_hot.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el target a un tensor</span></span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> torch.LongTensor(targets_idx)</span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el Forward </span></span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs = </span></span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> net(inputs_one_hot)</span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida</span></span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss =</span></span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Use el criterion definido arriba</span></span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, targets_idx)</span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a>        epoch_validation_loss <span class="op">+=</span> loss.detach().numpy()</span>
<span id="cb52-55"><a href="#cb52-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-56"><a href="#cb52-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NOTA 2</span></span>
<span id="cb52-57"><a href="#cb52-57" aria-hidden="true" tabindex="-1"></a>    net.train()</span>
<span id="cb52-58"><a href="#cb52-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-59"><a href="#cb52-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el training set</span></span>
<span id="cb52-60"><a href="#cb52-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> training_set:</span>
<span id="cb52-61"><a href="#cb52-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-62"><a href="#cb52-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb52-63"><a href="#cb52-63" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb52-64"><a href="#cb52-64" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> [word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> targets]</span>
<span id="cb52-65"><a href="#cb52-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-66"><a href="#cb52-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el input a un tensor</span></span>
<span id="cb52-67"><a href="#cb52-67" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> torch.Tensor(inputs_one_hot)</span>
<span id="cb52-68"><a href="#cb52-68" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> inputs_one_hot.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb52-69"><a href="#cb52-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-70"><a href="#cb52-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el target a un tensor</span></span>
<span id="cb52-71"><a href="#cb52-71" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> torch.LongTensor(targets_idx)</span>
<span id="cb52-72"><a href="#cb52-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-73"><a href="#cb52-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el Forward </span></span>
<span id="cb52-74"><a href="#cb52-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs = </span></span>
<span id="cb52-75"><a href="#cb52-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb52-76"><a href="#cb52-76" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> net(inputs_one_hot)</span>
<span id="cb52-77"><a href="#cb52-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb52-78"><a href="#cb52-78" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-79"><a href="#cb52-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida</span></span>
<span id="cb52-80"><a href="#cb52-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss =</span></span>
<span id="cb52-81"><a href="#cb52-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Use el criterion definido arriba</span></span>
<span id="cb52-82"><a href="#cb52-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb52-83"><a href="#cb52-83" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, targets_idx)</span>
<span id="cb52-84"><a href="#cb52-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb52-85"><a href="#cb52-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-86"><a href="#cb52-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 3 lineas para definir el backward</span></span>
<span id="cb52-87"><a href="#cb52-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># optimizer.</span></span>
<span id="cb52-88"><a href="#cb52-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss.</span></span>
<span id="cb52-89"><a href="#cb52-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># optimizer.</span></span>
<span id="cb52-90"><a href="#cb52-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb52-91"><a href="#cb52-91" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb52-92"><a href="#cb52-92" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb52-93"><a href="#cb52-93" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb52-94"><a href="#cb52-94" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span>
<span id="cb52-95"><a href="#cb52-95" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-96"><a href="#cb52-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb52-97"><a href="#cb52-97" aria-hidden="true" tabindex="-1"></a>        epoch_training_loss <span class="op">+=</span> loss.detach().numpy()</span>
<span id="cb52-98"><a href="#cb52-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-99"><a href="#cb52-99" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guardar la perdida para ser graficada</span></span>
<span id="cb52-100"><a href="#cb52-100" aria-hidden="true" tabindex="-1"></a>    training_loss.append(epoch_training_loss<span class="op">/</span><span class="bu">len</span>(training_set))</span>
<span id="cb52-101"><a href="#cb52-101" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(epoch_validation_loss<span class="op">/</span><span class="bu">len</span>(validation_set))</span>
<span id="cb52-102"><a href="#cb52-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-103"><a href="#cb52-103" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada 5 epocas</span></span>
<span id="cb52-104"><a href="#cb52-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb52-105"><a href="#cb52-105" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&#39;Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, training loss: </span><span class="sc">{</span>training_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, validation loss: </span><span class="sc">{</span>validation_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 0, training loss: 1.348198813199997, validation loss: 1.4144882917404176
Epoch 10, training loss: 0.5598714828491211, validation loss: 0.5109817296266556
Epoch 20, training loss: 0.41817260831594466, validation loss: 0.3748889058828354
Epoch 30, training loss: 0.36341856271028516, validation loss: 0.3150088548660278
Epoch 40, training loss: 0.33668678179383277, validation loss: 0.293888683617115
Epoch 50, training loss: 0.3205028915777802, validation loss: 0.28407980799674987
Epoch 60, training loss: 0.31017738599330186, validation loss: 0.274866783618927
Epoch 70, training loss: 0.304358615167439, validation loss: 0.2709150478243828
Epoch 80, training loss: 0.3286062298342586, validation loss: 0.3390388160943985
Epoch 90, training loss: 0.29831806998699906, validation loss: 0.2679189458489418
Epoch 100, training loss: 0.29660330787301065, validation loss: 0.2667516052722931
Epoch 110, training loss: 0.29525244552642105, validation loss: 0.26602641940116883
Epoch 120, training loss: 0.2942402884364128, validation loss: 0.26556051671504977
Epoch 130, training loss: 0.29346368424594405, validation loss: 0.26726687103509905
Epoch 140, training loss: 0.2929681332781911, validation loss: 0.26610356420278547
Epoch 150, training loss: 0.29263029955327513, validation loss: 0.26574755311012266
Epoch 160, training loss: 0.29232770670205355, validation loss: 0.26576957702636717
Epoch 170, training loss: 0.292026955075562, validation loss: 0.26603140532970426
Epoch 180, training loss: 0.29174363501369954, validation loss: 0.2663534492254257
Epoch 190, training loss: 0.29085528254508974, validation loss: 0.2681730806827545
Epoch 200, training loss: 0.29083405788987876, validation loss: 0.2672805920243263
Epoch 210, training loss: 0.2908763142302632, validation loss: 0.2671742781996727
Epoch 220, training loss: 0.29087852481752635, validation loss: 0.26734518110752103
Epoch 230, training loss: 0.2908420303836465, validation loss: 0.2676637753844261
Epoch 240, training loss: 0.2907707057893276, validation loss: 0.26804947555065156
Epoch 250, training loss: 0.33269670959562064, validation loss: 0.2687272742390633
Epoch 260, training loss: 0.2899562789127231, validation loss: 0.2693792715668678
Epoch 270, training loss: 0.29008453730493783, validation loss: 0.2691534891724586
Epoch 280, training loss: 0.29016607981175185, validation loss: 0.26927988678216935
Epoch 290, training loss: 0.2902008067816496, validation loss: 0.2695646956562996
Epoch 300, training loss: 0.2901948308572173, validation loss: 0.2699388787150383
Epoch 310, training loss: 0.2901601163670421, validation loss: 0.27037187069654467
Epoch 320, training loss: 0.29010991640388967, validation loss: 0.2707863196730614
Epoch 330, training loss: 0.2911234933882952, validation loss: 0.2768925681710243
Epoch 340, training loss: 0.28946690894663335, validation loss: 0.2725307419896126
Epoch 350, training loss: 0.28951781447976827, validation loss: 0.27186423540115356
Epoch 360, training loss: 0.2895985659211874, validation loss: 0.2718219250440598
Epoch 370, training loss: 0.2896580878645182, validation loss: 0.2720210999250412
Epoch 380, training loss: 0.2896872444078326, validation loss: 0.27232115119695666
Epoch 390, training loss: 0.28969427235424516, validation loss: 0.27265151143074035
Epoch 400, training loss: 0.2896821966394782, validation loss: 0.2729654982686043
Epoch 410, training loss: 0.2896499678492546, validation loss: 0.27324529737234116
Epoch 420, training loss: 0.2895962970331311, validation loss: 0.2735102534294128
Epoch 430, training loss: 0.2887963453307748, validation loss: 0.27459167689085007
Epoch 440, training loss: 0.28891611453145744, validation loss: 0.27437804043293
Epoch 450, training loss: 0.28903115522116424, validation loss: 0.27429051101207735
Epoch 460, training loss: 0.28911731000989677, validation loss: 0.27426301389932634
Epoch 470, training loss: 0.2891741365194321, validation loss: 0.27427901774644853
Epoch 480, training loss: 0.2892046818509698, validation loss: 0.27432120144367217
Epoch 490, training loss: 0.28921155761927364, validation loss: 0.27437656074762345
</code></pre>
</div>
</div>
<div id="0ca199e6" class="cell code" data-execution_count="33"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:00.769534Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:00.753913Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;18a505ffb2aa6222c3894bc5fee82e02&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-acfe6153f9006b27&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(training_loss[<span class="op">-</span><span class="dv">1</span>]), <span class="st">&quot;3c3d&quot;</span>, <span class="st">&#39;0x1.28f5c28f5c28fp-2&#39;</span>)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(validation_loss[<span class="op">-</span><span class="dv">1</span>]), <span class="st">&quot;3c3d&quot;</span>, <span class="st">&#39;0x1.28f5c28f5c28fp-2&#39;</span>)</span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div id="6561162c" class="cell code" data-execution_count="34"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:00.925207Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:00.769534Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5847ed5bbead7e432e5e12d4eb6114a3&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-3e1bfd6f4ff9568e&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la primera secuencia del test set</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode el input y el target</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>targets_idx <span class="op">=</span> [word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> targets]</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir el input a un tensor</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> torch.Tensor(inputs_one_hot)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> inputs_one_hot.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir el target a un tensor</span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>targets_idx <span class="op">=</span> torch.LongTensor(targets_idx)</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 1 linea para el Forward </span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs = </span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> net(inputs_one_hot)</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> F.softmax(outputs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a><span class="co">#raise NotImplementedError()</span></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Input:&quot;</span>)</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Target:&quot;</span>)</span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&quot;Secuencia Predicha:&quot;)</span></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a><span class="co"># print([idx_to_word[np.argmax(output)] for output in outputs])</span></span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Secuencia Predicha:&quot;</span>)</span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output.detach().numpy())] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar la perdida en training y validacion</span></span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(training_loss))</span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, training_loss, <span class="st">&#39;r&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training loss&#39;</span>,)</span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, validation_loss, <span class="st">&#39;b&#39;</span>, label<span class="op">=</span><span class="st">&#39;Validation loss&#39;</span>)</span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>), plt.ylabel(<span class="st">&#39;NLL&#39;</span>)</span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Secuencia Input:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;]
Secuencia Target:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
Secuencia Predicha:
[&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;EOS&#39;]
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_c857c590cd71462387f9b785ba6f5f7d/1f3fb9a02ffac06c90b5cefd6cc61d45ea43e297.png" /></p>
</div>
</div>
<div id="ee03eeae" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;f80da25b1e5ffd177becd68eb2c2dde2&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1fb0f402aab24ee3&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h4 id="preguntas">Preguntas</h4>
<p><strong>Responda</strong> lo siguiente dentro de esta celda</p>
<ul>
<li>Compare las graficas obtenidas en el LSTM "a mano" y el LSTM "usando
PyTorch, ¿cuál cree que es mejor? ¿Por qué?</li>
</ul>
<p><br/><span style="color:skyblue"> R/. Para poder comparalas de manera
más cómoda, se agregan a continuación con el fin de tener fácil acceso a
estas</span> <br/><br/></p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>LSTM "a mano"</th>
<th>LSTM "usando pytorch"</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="vertopal_c857c590cd71462387f9b785ba6f5f7d/image.png"
alt="image.png" /></td>
<td><img src="vertopal_c857c590cd71462387f9b785ba6f5f7d/image-2.png"
alt="image-2.png" /></td>
</tr>
</tbody>
</table>
<p><br/><span style="color:skyblue"> Inmediatamente se puede ver una
diferencia considerable en lo que a corelacion entre "Training loss" y
"Validation loss" se refiere, por lo que se puede argumentar que LSTM
"usando pytorch" es mejor.<br/><br/> Esto se puede deber a los niveles
de precisión que maneja la liberia a comparación de los calculos en
python, además de que en ciertas partes de los procesos a mano se puede
perder detalle.</span> <br/><br/></p>
<ul>
<li><p>Compare la secuencia target y la predicha de esta parte, ¿en qué
parte falló el modelo? <br/><br/><span style="color:skyblue"> R/. En los
outputs de la celda anterior, se puede apreciar que esta falla en lo que
corresponde al punto en que las 'a's se convierten en 'b's</span>
<br/><br/></p></li>
<li><p>¿Qué sucede en el código donde se señala "NOTA 1" y "NOTA 2"?
¿Para qué son necesarias estas líneas?
<br/><br/><span style="color:skyblue"> R/. Mientras que "NOTA 1"
corresponde al proceso de recibir el input y target y pasarlo a un
tensor y de ahi recibir la actualización de perdida para cada secuencia
en el validation set, "NOTA 2" corresponde al mismo proceso pero para
cada secuencia dentro del training set</span> <br/><br/></p></li>
</ul>
</div>
<div id="55ccbc27" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;772e173eeac39b0919121141a48d2484&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5bb7d8b7aa5b0eba&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<h2 id="parte-4---segunda-red-neuronal-lstm-con-pytorch">Parte 4 -
Segunda Red Neuronal LSTM con PyTorch</h2>
<p>Para esta parte será un poco menos guiada, por lo que se espera que
puedan generar un modelo de Red Neuronal con LSTM para solventar un
problema simple. Lo que se evaluará es la métrica final, y solamente se
dejarán las generalidades de la implementación. El objetivo de esta
parte, es dejar que ustedes exploren e investiguen un poco más por su
cuenta.</p>
<p>En este parte haremos uso de las redes LSTM pero para predicción de
series de tiempo. Entonces lo que se busca es que dado un mes y un año,
se debe predecir el número de pasajeros en unidades de miles. Los datos
a usar son de 1949 a 1960.</p>
<p>Basado del blog "LSTM for Time Series Prediction in PyTorch" de
Adrian Tam.</p>
</div>
<div id="a0f4e03a" class="cell code" data-execution_count="35"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:00.949969Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:00.927201Z&quot;}">
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seed all</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>random.seed(seed_)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed_)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed(seed_)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed_)  <span class="co"># Multi-GPU.</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span></code></pre></div>
</div>
<div id="b5b161e6" class="cell code" data-execution_count="36"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:04.694521Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:00.951962Z&quot;}">
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>url_data <span class="op">=</span> <span class="st">&quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv&quot;</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> pd.read_csv(url_data)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>dataset.head(<span class="dv">10</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="36">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Month</th>
      <th>Passengers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1949-01</td>
      <td>112</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1949-02</td>
      <td>118</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1949-03</td>
      <td>132</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1949-04</td>
      <td>129</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1949-05</td>
      <td>121</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1949-06</td>
      <td>135</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1949-07</td>
      <td>148</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1949-08</td>
      <td>148</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1949-09</td>
      <td>136</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1949-10</td>
      <td>119</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div id="39147fe4" class="cell code" data-execution_count="37"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:04.817904Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:04.694521Z&quot;}">
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dibujemos la serie de tiempo</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>time_series <span class="op">=</span> dataset[[<span class="st">&quot;Passengers&quot;</span>]].values.astype(<span class="st">&#39;float32&#39;</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>plt.plot(time_series)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_c857c590cd71462387f9b785ba6f5f7d/118371bb724d90c66c8179565ef4b057a0204df8.png" /></p>
</div>
</div>
<div id="26c0a974" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;2b571f3d3e711cd00704160b9076470c&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-62ab455036fa4a55&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<p>Esta serie de tiempo comprende 144 pasos de tiempo. El gráfico indica
claramente una tendencia al alza y hay patrones periódicos en los datos
que corresponden al período de vacaciones de verano. Por lo general, se
recomienda "eliminar la tendencia" de la serie temporal eliminando el
componente de tendencia lineal y normalizándolo antes de continuar con
el procesamiento. Sin embargo, por simplicidad de este ejercicios, vamos
a omitir estos pasos.</p>
<p>Ahora necesitamos dividir nuestro dataset en training, validation y
test set. A diferencia de otro tipo de datasets, cuando se trabaja en
este tipo de proyectos, la división se debe hacer sin "revolver" los
datos. Para esto, podemos hacerlo con NumPy</p>
</div>
<div id="386a179c" class="cell code" data-execution_count="38"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:04.840674Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:04.817904Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;07cb1e706347a5e56eac2633b37bcaf1&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-35af372f0bf820a2&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb60"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># En esta ocasion solo usaremos train y test, validation lo omitiremos para simpleza del ejercicio</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># NO CAMBIEN NADA DE ESTA CELDA POR FAVOR</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>p_train<span class="op">=</span><span class="fl">0.8</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>p_test<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Definimos el tamaño de las particiones</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>num_train <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(time_series)<span class="op">*</span>p_train)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>num_test <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(time_series)<span class="op">*</span>p_test)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir las secuencias en las particiones</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> time_series[:num_train]</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> time_series[num_train:]</span></code></pre></div>
</div>
<div id="50b657bd" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;40c1e60513e029a06d25435af49dad3a&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-ece3e13c7a8ed477&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<p>El aspecto más complicado es determinar el método por el cual la red
debe predecir la serie temporal. Por lo general, la predicción de series
temporales se realiza en función de una ventana. En otras palabras,
recibe datos del tiempo t1 al t2, y su tarea es predecir para el tiempo
t3 (o más adelante). El tamaño de la ventana, denotado por w, dicta
cuántos datos puede considerar el modelo al hacer la predicción. Este
parámetro también se conoce como <strong>look back period</strong>
(período retrospectivo).</p>
<p>Entonces, creemos una función para obtener estos datos, dado un look
back period. Además, debemos asegurarnos de transformar estos datos a
tensores para poder ser usados con PyTorch.</p>
<p>Esta función está diseñada para crear ventanas en la serie de tiempo
mientras predice un paso de tiempo en el futuro inmediato. Su propósito
es convertir una serie de tiempo en un tensor con dimensiones (muestras
de ventana, pasos de tiempo, características). Dada una serie de tiempo
con t pasos de tiempo, puede producir aproximadamente (t - ventana + 1)
ventanas, donde "ventana" denota el tamaño de cada ventana. Estas
ventanas pueden comenzar desde cualquier paso de tiempo dentro de la
serie de tiempo, siempre que no se extiendan más allá de sus
límites.</p>
<p>Cada ventana contiene múltiples pasos de tiempo consecutivos con sus
valores correspondientes, y cada paso de tiempo puede tener múltiples
características. Sin embargo, en este conjunto de datos específico, solo
hay una función disponible.</p>
<p>La elección del diseño garantiza que tanto la "característica" como
el "objetivo" tengan la misma forma. Por ejemplo, para una ventana de
tres pasos de tiempo, la "característica" corresponde a la serie de
tiempo de t-3 a t-1, y el "objetivo" cubre los pasos de tiempo de t-2 a
t. Aunque estamos principalmente interesados en predecir t+1, la
información de t-2 a t es valiosa durante el entrenamiento.</p>
<p>Es importante tener en cuenta que la serie temporal de entrada se
representa como una matriz 2D, mientras que la salida de la función
<code>create_timeseries_dataset()</code> será un tensor 3D. Para
demostrarlo, usemos lookback=1 y verifiquemos la forma del tensor de
salida en consecuencia.</p>
</div>
<div id="2ae6e2f8" class="cell code" data-execution_count="39"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:04.862500Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:04.842686Z&quot;}">
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_timeseries_dataset(dataset, lookback):</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> [], []</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dataset) <span class="op">-</span> lookback):</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        feature <span class="op">=</span> dataset[i : i <span class="op">+</span> lookback]</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> dataset[i <span class="op">+</span> <span class="dv">1</span> : i <span class="op">+</span> lookback <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        X.append(feature)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        y.append(target)</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(X), torch.tensor(y)</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="co"># EL VALOR DE LB SÍ LO PUEDEN CAMBIAR SI LO CONSIDERAN NECESARIO</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>lb <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> create_timeseries_dataset(train, lookback<span class="op">=</span>lb)</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co">#X_validation, y_validation = create_timeseries_dataset(validation, lookback=lb)</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> create_timeseries_dataset(test, lookback<span class="op">=</span>lb)</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, y_train.shape)</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="co">#print(X_validation.shape, y_validation.shape)</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_test.shape, y_test.shape)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>torch.Size([111, 4, 1]) torch.Size([111, 4, 1])
torch.Size([25, 4, 1]) torch.Size([25, 4, 1])
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>C:\Users\charl\AppData\Local\Temp\ipykernel_43384\2018909527.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\torch\csrc\utils\tensor_new.cpp:248.)
  return torch.tensor(X), torch.tensor(y)
</code></pre>
</div>
</div>
<div id="736f894c" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;d10291404d48c7939620e98bdf5c78c9&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-23fc69181d7a7cd8&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<p>Ahora necesitamos crear una clase que definirá nuestro modelo de red
neuronal con LSTM. Noten que acá solo se dejaran las firmas de las
funciones necesarias, ustedes deberán decidir que arquitectura con LSTM
implementar, con la finalidad de superar cierto threshold de métrica de
desempeño mencionado abajo.</p>
</div>
<div id="dfb5df7a" class="cell code" data-execution_count="40"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:35:04.893730Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:04.862500Z&quot;}"
data-deletable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;51bcc393e21e6cbb4e8535556d11e975&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-f0f68d3f484736df&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb64"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co"># NOTA: Moví el numero de iteraciones para que no se borre al ser evaluado</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Pueden cambiar el número de epocas en esta ocasión con tal de llegar al valor de la metrica de desempeño</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">4000</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co"># raise NotImplementedError()</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="co"># class CustomModelLSTM(nn.Module):</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self):</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="co">#         # YOUR CODE HERE</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co">#         # raise NotImplementedError()</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.linear = nn.Linear(50, 1)</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x):</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="co">#         # YOUR CODE HERE</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="co">#         # raise NotImplementedError()</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="co">#         # return x</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a><span class="co">#         x, _ = self.lstm(x)</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="co">#         x = self.linear(x)</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a><span class="co">#         return x</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomModelLSTM(nn.Module):</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>        input_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>        hidden_size <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>        num_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>        bidirectional <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(input_size<span class="op">=</span>input_size, hidden_size<span class="op">=</span>hidden_size, num_layers<span class="op">=</span>num_layers, batch_first<span class="op">=</span><span class="va">True</span>, bidirectional<span class="op">=</span>bidirectional)</span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.2</span>)</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(hidden_size <span class="op">*</span> <span class="dv">2</span> <span class="cf">if</span> bidirectional <span class="cf">else</span> hidden_size, <span class="dv">1</span>)</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="va">self</span>.lstm(x)</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear(x)</span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div id="ca183d4b" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;6a4476b61104b249dbdf1098ff92545f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-e023e0bb22dd42ad&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<p>La función nn.LSTM() produce una tupla como salida. El primer
elemento de esta tupla consiste en los hidden states generados, donde
cada paso de tiempo de la entrada tiene su correspondiente hidden state.
El segundo elemento contiene la memoria y los hidden states de la unidad
LSTM, pero no se usan en este contexto particular.</p>
<p>La capa LSTM se configura con la opción <code>batch_first=True</code>
porque los tensores de entrada se preparan en la dimensión de (muestra
de ventana, pasos de tiempo, características). Con esta configuración,
se crea un batch tomando muestras a lo largo de la primera
dimensión.</p>
<p>Para generar un único resultado de regresión, la salida de los
estados ocultos se procesa aún más utilizando una capa fully connected.
Dado que la salida de LSTM corresponde a un valor para cada paso de
tiempo de entrada, se debe seleccionar solo la salida del último paso de
tiempo.</p>
</div>
<div id="11ae7532" class="cell code" data-execution_count="41"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:36:19.729245Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:35:04.893730Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;cce72799bead411086daec37631d789e&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-d106920d76b987cc&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co"># NOTEN QUE ESTOY PONIENDO DE NUEVO LOS SEEDS PARA SER CONSTANTES</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>random.seed(seed_)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed_)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed(seed_)</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed_)  <span class="co"># Multi-GPU.</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a><span class="co">############</span></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CustomModelLSTM()</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizador y perdida</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters())</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Observen como podemos también definir un DataLoader de forma snecilla</span></span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> data.DataLoader(data.TensorDataset(X_train, y_train), shuffle<span class="op">=</span><span class="va">False</span>, batch_size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Perdidas</span></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>loss_train <span class="op">=</span> []</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>loss_test <span class="op">=</span> []</span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos sobre cada epoca</span></span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Colocamos el modelo en modo de entrenamiento</span></span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cargamos los batches</span></span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> loader:</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Obtenemos una primera prediccion</span></span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_batch)</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculamos la perdida</span></span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y_batch)</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reseteamos la gradiente a cero</span></span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   sino la gradiente de previas iteraciones se acumulará con las nuevas</span></span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop</span></span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aplicar las gradientes para actualizar los parametros del modelo</span></span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validación cada 100 epocas</span></span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">!=</span> <span class="dv">0</span> <span class="kw">and</span> epoch <span class="op">!=</span> n_epochs<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Colocamos el modelo en modo de evaluación</span></span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Deshabilitamos el calculo de gradientes</span></span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prediccion</span></span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_train)</span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del RMSE - Root Mean Square Error</span></span>
<span id="cb65-57"><a href="#cb65-57" aria-hidden="true" tabindex="-1"></a>        train_rmse <span class="op">=</span> np.sqrt(loss_fn(y_pred, y_train))</span>
<span id="cb65-58"><a href="#cb65-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prediccion sobre validation</span></span>
<span id="cb65-59"><a href="#cb65-59" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_test)</span>
<span id="cb65-60"><a href="#cb65-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del RMSE para validation</span></span>
<span id="cb65-61"><a href="#cb65-61" aria-hidden="true" tabindex="-1"></a>        test_rmse <span class="op">=</span> np.sqrt(loss_fn(y_pred, y_test))</span>
<span id="cb65-62"><a href="#cb65-62" aria-hidden="true" tabindex="-1"></a>        loss_train.append(train_rmse)</span>
<span id="cb65-63"><a href="#cb65-63" aria-hidden="true" tabindex="-1"></a>        loss_test.append(test_rmse)</span>
<span id="cb65-64"><a href="#cb65-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-65"><a href="#cb65-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">%d</span><span class="st">: train RMSE </span><span class="sc">%.4f</span><span class="st">, test RMSE </span><span class="sc">%.4f</span><span class="st">&quot;</span> <span class="op">%</span> (epoch, train_rmse, test_rmse))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 0: train RMSE 254.1170, test RMSE 448.9859
Epoch 100: train RMSE 150.3338, test RMSE 335.1707
Epoch 200: train RMSE 101.8262, test RMSE 267.1939
Epoch 300: train RMSE 71.8909, test RMSE 224.2183
Epoch 400: train RMSE 52.7294, test RMSE 181.5784
Epoch 500: train RMSE 35.2587, test RMSE 150.2749
Epoch 600: train RMSE 27.1761, test RMSE 127.5523
Epoch 700: train RMSE 22.8643, test RMSE 110.9470
Epoch 800: train RMSE 21.1688, test RMSE 99.4444
Epoch 900: train RMSE 18.1533, test RMSE 90.9114
Epoch 1000: train RMSE 16.7192, test RMSE 85.2862
Epoch 1100: train RMSE 22.1860, test RMSE 81.2006
Epoch 1200: train RMSE 14.6047, test RMSE 77.4486
Epoch 1300: train RMSE 14.7741, test RMSE 74.3524
Epoch 1400: train RMSE 13.8799, test RMSE 71.8381
Epoch 1500: train RMSE 13.7004, test RMSE 70.0551
Epoch 1600: train RMSE 13.7066, test RMSE 68.9328
Epoch 1700: train RMSE 13.2695, test RMSE 64.9038
Epoch 1800: train RMSE 14.6497, test RMSE 65.2097
Epoch 1900: train RMSE 12.4337, test RMSE 62.3382
Epoch 2000: train RMSE 12.0386, test RMSE 60.9419
Epoch 2100: train RMSE 14.0973, test RMSE 62.8782
Epoch 2200: train RMSE 18.1339, test RMSE 68.2894
Epoch 2300: train RMSE 12.5044, test RMSE 60.1656
Epoch 2400: train RMSE 11.3381, test RMSE 59.1912
Epoch 2500: train RMSE 12.0881, test RMSE 59.3898
Epoch 2600: train RMSE 11.2627, test RMSE 61.3556
Epoch 2700: train RMSE 13.3345, test RMSE 58.5820
Epoch 2800: train RMSE 10.9882, test RMSE 60.9149
Epoch 2900: train RMSE 12.9894, test RMSE 60.0054
Epoch 3000: train RMSE 10.9358, test RMSE 60.7575
Epoch 3100: train RMSE 11.3355, test RMSE 58.2408
Epoch 3200: train RMSE 13.3795, test RMSE 63.3641
Epoch 3300: train RMSE 10.3484, test RMSE 59.9070
Epoch 3400: train RMSE 11.0857, test RMSE 59.9422
Epoch 3500: train RMSE 10.1330, test RMSE 58.5551
Epoch 3600: train RMSE 10.8447, test RMSE 59.6736
Epoch 3700: train RMSE 9.7848, test RMSE 57.2224
Epoch 3800: train RMSE 10.2627, test RMSE 57.5288
Epoch 3900: train RMSE 9.6011, test RMSE 58.7403
Epoch 3999: train RMSE 10.0920, test RMSE 58.6863
</code></pre>
</div>
</div>
<div id="ec8794e6" class="cell code" data-execution_count="42"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:36:19.885458Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:36:19.729245Z&quot;}">
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización del rendimiento</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(loss_train))</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, loss_train, <span class="st">&#39;r&#39;</span>, label<span class="op">=</span><span class="st">&#39;Training&#39;</span>,)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, loss_test, <span class="st">&#39;b&#39;</span>, label<span class="op">=</span><span class="st">&#39;Test&#39;</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>), plt.ylabel(<span class="st">&#39;RMSE&#39;</span>)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_c857c590cd71462387f9b785ba6f5f7d/91fccd03fe801ed9774684c6d258bb21f64bf6ea.png" /></p>
</div>
</div>
<div id="7b4eae30" class="cell code" data-execution_count="43"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:36:20.041672Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:36:19.885458Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;52fe33653ffb1624968f4a4a8b8dd877&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5a5264aa04158cad&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb68"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficamos</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Movemos las predicciones de train para graficar</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    train_plot <span class="op">=</span> np.ones_like(time_series) <span class="op">*</span> np.nan</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prediccion de train</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X_train)</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extraemos los datos solo del ultimo paso</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> y_pred[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    train_plot[lb : num_train] <span class="op">=</span> model(X_train)[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Movemos las predicciones de test</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    test_plot <span class="op">=</span> np.ones_like(time_series) <span class="op">*</span> np.nan</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    test_plot[num_train <span class="op">+</span> lb : <span class="bu">len</span>(time_series)] <span class="op">=</span> model(X_test)[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>plt.plot(time_series, label<span class="op">=</span><span class="st">&quot;Serie Original&quot;</span>)</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>plt.plot(train_plot, c<span class="op">=</span><span class="st">&#39;r&#39;</span>, label<span class="op">=</span><span class="st">&quot;Serie Train&quot;</span>)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>plt.plot(test_plot, c<span class="op">=</span><span class="st">&#39;g&#39;</span>, label<span class="op">=</span><span class="st">&quot;Serie Test&quot;</span>)</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Paso en el Tiempo&#39;</span>), plt.ylabel(<span class="st">&#39;Pasajeros&#39;</span>)</span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_c857c590cd71462387f9b785ba6f5f7d/ddacbb5110ea66a5c3f940e885ac7a82ffe038e1.png" /></p>
</div>
</div>
<div id="473ff4a8" class="cell markdown" data-deletable="false"
data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;150fbfe9209ee5b1fc82c08094ee43fd&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-7a20e9d17f776c79&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<p><strong>Nota:</strong> Lo que se estará evaluando es el RMSE tanto en
training como en test. Se evaluará que en training sea <strong>menor a
22</strong>, mientras que en testing sea <strong>menor a
70</strong>.</p>
</div>
<div id="f8ac2320" class="cell code" data-execution_count="44"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:36:20.056981Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:36:20.043017Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;04af852d7a882ae7a5dddcd4fe42d22b&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-65c8e80376d46bc1&quot;,&quot;locked&quot;:true,&quot;points&quot;:28,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb69"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="bu">float</span>(loss_test[<span class="bu">len</span>(loss_test)<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="bu">float</span>(test_rmse)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>loss_train</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> loss_train[<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;</span> <span class="dv">22</span> </span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> train_rmse <span class="op">&lt;</span> <span class="dv">22</span> </span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> loss_test[<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;</span> <span class="dv">70</span> </span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> test_rmse <span class="op">&lt;</span> <span class="dv">70</span> </span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>    </span></code></pre></div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
<div class="output display_data">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
</div>
<div id="0f8e00b4" class="cell code" data-execution_count="45"
data-ExecuteTime="{&quot;end_time&quot;:&quot;2023-08-05T23:36:20.074671Z&quot;,&quot;start_time&quot;:&quot;2023-08-05T23:36:20.057977Z&quot;}"
data-deletable="false" data-editable="false"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5fc71d80805acbbec919a3972572b7f4&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-a895611caee19d78&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb70"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio&quot;</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>tick.summarise_marks() <span class="co"># </span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>
La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio
</code></pre>
</div>
<div class="output display_data">
<!--{id:"TOTALMARK",marks:"158", available:"158"}  -->
        
        <h1> 158 / 158 marks (100.0%) </h1>
        
</div>
</div>
<div id="1008a216" class="cell code">
<div class="sourceCode" id="cb72"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
</body>
</html>
